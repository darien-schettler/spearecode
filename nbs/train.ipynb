{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0acb0112-c7aa-4f44-b9f2-b00adce49d8e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b>Imports and Constants</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4bf6c3-8590-4e25-95f7-24a62edd1c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 17:36:04.853977: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-14 17:36:05.462635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q --upgrade tokenizer-viz\n",
    "\n",
    "# Regular imports (native python and pypi packages)\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from IPython.display import HTML, display\n",
    "from tokenizer_viz import TokenVisualization\n",
    "from tqdm.notebook import tqdm; tqdm.pandas()\n",
    "\n",
    "# Add project root into path so imports work\n",
    "PROJECT_DIR = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, PROJECT_DIR) \n",
    "\n",
    "# Our project imports\n",
    "from spearecode.utils.preprocessing_utils import (\n",
    "    load_from_txt_file, preprocess_shakespeare, save_to_txt_file, print_check_speare, get_spm_assets\n",
    ")\n",
    "from spearecode.utils.general_utils import (\n",
    "    tf_xla_jit, tf_set_memory_growth, seed_it_all, flatten_l_o_l, print_ln\n",
    ")\n",
    "from spearecode.utils.filtering_utils import (\n",
    "    save_ds_version, drop_str_from_col_names, pad_truncate_centered,\n",
    "    get_metadata_df, check_chunks, tokenize, get_n_tokens,\n",
    "    get_n_lines, get_n_chars\n",
    ")\n",
    "from spearecode.utils.tfrecord_utils import write_tfrecords, load_tfrecord_dataset\n",
    "\n",
    "TRAIN_STYLE = \"rcts_bpe_v4\"\n",
    "CHUNK_STYLE, TOK_STYLE, DS_VERSION = TRAIN_STYLE.split(\"_\")\n",
    "\n",
    "### DEFINE PATHS --- [PROJECT_DIR=\"/home/paperspace/home/spearecode\"] --- ###\n",
    "NBS_PATH = os.path.join(PROJECT_DIR, \"nbs\")\n",
    "DATA_PATH = os.path.join(PROJECT_DIR, \"data\")\n",
    "SS_TEXT_PATH = os.path.join(DATA_PATH, \"t8.shakespeare.txt\")\n",
    "PREPROCESSED_FULL_TEXT_PATH = SS_TEXT_PATH.replace(\".txt\", \"_preprocessed.txt\")\n",
    "\n",
    "DATASETS_PATH = os.path.join(DATA_PATH, \"datasets\") \n",
    "META_DIR = os.path.join(DATASETS_PATH, \"meta\") \n",
    "TFRECORD_DIR = os.path.join(DATASETS_PATH, \"tfrecords\", TRAIN_STYLE)\n",
    "MODELS_DIR = os.path.join(PROJECT_DIR, \"models\")\n",
    "\n",
    "# Specific Paths\n",
    "SPM_MODEL_PATH = os.path.join(MODELS_DIR, f\"spearecode_{TOK_STYLE}\")\n",
    "DATA_CSV_PATH  = os.path.join(DATASETS_PATH, f\"{DS_VERSION}_{CHUNK_STYLE}_{TOK_STYLE}.csv\")\n",
    "META_CSV_PATH  = os.path.join(META_DIR, f\"{DS_VERSION}_{CHUNK_STYLE}_{TOK_STYLE}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2accc846-874c-4bfc-9bef-b01ca883f5a1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b>Instantiate expected tools for the reset of the notebook</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2dec0a8-a181-492a-ac75-f397776c522c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>token_content</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>n_lines</th>\n",
       "      <th>valid_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1\\n  From fairest creatures we desire increase...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>192</td>\n",
       "      <td>643</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2\\n  When forty winters shall besiege thy brow...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>188</td>\n",
       "      <td>662</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3\\n  Look in thy glass and tell the face thou ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>183</td>\n",
       "      <td>643</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4\\n  Unthrifty loveliness why dost thou spend,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>183</td>\n",
       "      <td>619</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5\\n  Those hours that with gentle work did fra...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>176</td>\n",
       "      <td>652</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7694</th>\n",
       "      <td>'\"Lo, this device was sent me from a nun,\\n  O...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>282</td>\n",
       "      <td>944</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7695</th>\n",
       "      <td>'\"How mighty then you are, O hear me tell!\\n  ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>298</td>\n",
       "      <td>983</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7696</th>\n",
       "      <td>'\"Now all these hearts that do on mine depend,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>283</td>\n",
       "      <td>977</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7697</th>\n",
       "      <td>'For lo, his passion, but an art of craft,\\n  ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>292</td>\n",
       "      <td>965</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7698</th>\n",
       "      <td>'Thus merely with the garment of a Grace  \\n  ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>199</td>\n",
       "      <td>630</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7699 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  \\\n",
       "0     1\\n  From fairest creatures we desire increase...   \n",
       "1     2\\n  When forty winters shall besiege thy brow...   \n",
       "2     3\\n  Look in thy glass and tell the face thou ...   \n",
       "3     4\\n  Unthrifty loveliness why dost thou spend,...   \n",
       "4     5\\n  Those hours that with gentle work did fra...   \n",
       "...                                                 ...   \n",
       "7694  '\"Lo, this device was sent me from a nun,\\n  O...   \n",
       "7695  '\"How mighty then you are, O hear me tell!\\n  ...   \n",
       "7696  '\"Now all these hearts that do on mine depend,...   \n",
       "7697  'For lo, his passion, but an art of craft,\\n  ...   \n",
       "7698  'Thus merely with the garment of a Grace  \\n  ...   \n",
       "\n",
       "                                          token_content  n_tokens  n_chars  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       192      643   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       188      662   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       183      643   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       183      619   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       176      652   \n",
       "...                                                 ...       ...      ...   \n",
       "7694  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       282      944   \n",
       "7695  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       298      983   \n",
       "7696  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       283      977   \n",
       "7697  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       292      965   \n",
       "7698  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       199      630   \n",
       "\n",
       "      n_lines  valid_chunk  \n",
       "0          15         True  \n",
       "1          15         True  \n",
       "2          15         True  \n",
       "3          15         True  \n",
       "4          15         True  \n",
       "...       ...          ...  \n",
       "7694       23         True  \n",
       "7695       23         True  \n",
       "7696       23         True  \n",
       "7697       23         True  \n",
       "7698       15         True  \n",
       "\n",
       "[7699 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>n_lines</th>\n",
       "      <th>valid_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192</td>\n",
       "      <td>643</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>662</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>183</td>\n",
       "      <td>643</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183</td>\n",
       "      <td>619</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176</td>\n",
       "      <td>652</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7694</th>\n",
       "      <td>282</td>\n",
       "      <td>944</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7695</th>\n",
       "      <td>298</td>\n",
       "      <td>983</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7696</th>\n",
       "      <td>283</td>\n",
       "      <td>977</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7697</th>\n",
       "      <td>292</td>\n",
       "      <td>965</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7698</th>\n",
       "      <td>199</td>\n",
       "      <td>630</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7699 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n_tokens  n_chars  n_lines  valid_chunk\n",
       "0          192      643       15         True\n",
       "1          188      662       15         True\n",
       "2          183      643       15         True\n",
       "3          183      619       15         True\n",
       "4          176      652       15         True\n",
       "...        ...      ...      ...          ...\n",
       "7694       282      944       23         True\n",
       "7695       298      983       23         True\n",
       "7696       283      977       23         True\n",
       "7697       292      965       23         True\n",
       "7698       199      630       15         True\n",
       "\n",
       "[7699 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>span.token {font-family: Courier New; font-size: 1.1em; font-weight: 300; padding: 0px; margin-right: 0px; border-color: rgba(0, 0, 0, 0.05); border-style: ridge; border-radius: 0px;}</style><div style='background-color: #FBFBFB; line-height: 175%; padding: 25px; border-radius: 8px; margin-left: 10px; margin-right: 10px; margin-top: 20px; margin-bottom: 20px; overflow-x: auto; white-space: nowrap;'><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>SCENE</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;II</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>S</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>aint</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;Alb</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>ans</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>Alarum</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>s</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;battle</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>.</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;Enter</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;WARWICK</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>WARWICK</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Clifford</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;C</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>umberland</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;'</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>tis</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;Warwick</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;calls</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>;</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>And</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;if</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;thou</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;dost</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;not</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;hide</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;thee</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;from</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;bear</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>,</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Now</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>,</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;when</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;angry</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;trumpet</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;sounds</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;alarum</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>And</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;dead</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;men</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>'</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>s</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;cries</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;do</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;fill</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;empty</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;air</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>,</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Cl</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ifford</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>,</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;say</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;come</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;forth</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;fight</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;with</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;me</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>.</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Pr</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>oud</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;n</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>ort</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>her</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>n</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;lord</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>,</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;Clifford</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;C</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>umberland</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>,</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>WARWICK</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;is</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;ho</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>ar</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>se</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;with</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;call</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>ing</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;thee</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;arms</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>.</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>Enter</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;YORK</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>How</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;now</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>,</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;noble</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;lord</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>!</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;what</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>,</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;all</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;a</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>-</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>foot</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>?</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>YORK</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;The</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;deadly</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>-</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>hand</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>ed</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;Clifford</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;slew</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;st</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>eed</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>;</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>But</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;match</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;match</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;have</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;enc</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>ount</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>'</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>red</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;him</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>,</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>And</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;made</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;a</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;prey</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;for</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;carri</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>on</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;k</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>ites</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;c</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>rows</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Even</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;bon</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>ny</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;beast</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;he</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;lov</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>'</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>d</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;so</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;well</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>.</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>Enter</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;O</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>L</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>D</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;CLIFFORD</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'><br></span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sp, encoder, decoder = get_spm_assets(SPM_MODEL_PATH)\n",
    "MASK_TOKEN_STR = \"[MASK]\"\n",
    "MASK_TOKEN_INT = encoder(MASK_TOKEN_STR)\n",
    "\n",
    "viz_tool = TokenVisualization(encoder, decoder, background_color=\"#FBFBFB\", transparency=0.4)\n",
    "train_df = pd.read_csv(DATA_CSV_PATH)\n",
    "meta_df  = pd.read_csv(META_CSV_PATH)\n",
    "\n",
    "display(train_df)\n",
    "display(meta_df)\n",
    "\n",
    "_ = viz_tool.visualize(train_df.content.sample(1).values[0], display_inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5570e-b19d-4de3-8014-9206f5271759",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b>Create Datasets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15f7871-e2ba-44c2-b4fb-974c7985ee3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 17:36:07.342866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 17:36:07.370873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 17:36:07.371071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 17:36:07.372083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 17:36:07.372239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 17:36:07.372352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 17:36:08.082887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 17:36:08.083088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 17:36:08.083232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 17:36:08.083356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13931 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<_MapDataset element_spec=TensorSpec(shape=(384,), dtype=tf.int64, name=None)>,\n",
       " <_MapDataset element_spec=TensorSpec(shape=(384,), dtype=tf.int64, name=None)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all tfrecords and shuffle\n",
    "ALL_TFRECORDS = glob(os.path.join(TFRECORD_DIR, \"*.tfrec\"))\n",
    "random.shuffle(ALL_TFRECORDS)\n",
    "N_TOTAL_RECS = len(ALL_TFRECORDS)\n",
    "\n",
    "\n",
    "EX_PER_TFREC = 100\n",
    "VAL_PCT = 0.125\n",
    "N_VAL_RECS = int(VAL_PCT*N_TOTAL_RECS)\n",
    "\n",
    "VAL_TFRECORDS = ALL_TFRECORDS[:N_VAL_RECS]\n",
    "TRAIN_TFRECORDS = ALL_TFRECORDS[N_VAL_RECS:]\n",
    "\n",
    "train_ds = load_tfrecord_dataset(TRAIN_TFRECORDS)\n",
    "val_ds = load_tfrecord_dataset(VAL_TFRECORDS)\n",
    "\n",
    "(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ab3af-9ce8-444f-943d-6944a3550184",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b>Training Configuration</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71bf0125-77d4-4f2f-91be-4807a3abcf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = dict(\n",
    "    batch_size=32,\n",
    "    shuffle_buffer=512,\n",
    "    encoder_context_len=128,\n",
    "    decoder_context_len=64,\n",
    "    mask_token_id=sp.encode(\"[MASK]\")[0],\n",
    "    vocab_size=sp.vocab_size(),\n",
    "    n_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c91c42-6f70-4430-85e8-eb7c4dbdeac8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b>TF.Data Pipeline</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddfef8be-bcba-4ee1-8208-3fda9babea73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_PrefetchDataset element_spec=((TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), TensorSpec(shape=(32, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), TensorSpec(shape=(32, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(32, 128), dtype=tf.float32, name=None), TensorSpec(shape=(32, 64), dtype=tf.float32, name=None)))>,\n",
       " <_PrefetchDataset element_spec=((TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), TensorSpec(shape=(32, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), TensorSpec(shape=(32, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(32, 128), dtype=tf.float32, name=None), TensorSpec(shape=(32, 64), dtype=tf.float32, name=None)))>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "# --- Pipeline Steps ---\n",
    "# \n",
    "# 1. Shuffle examples (shuffle_buffer)\n",
    "# 2. Batch examples (batch_size, drop_remainder, AUTOTUNE)\n",
    "# 3. Split sequence into encoder/decoder inputs [`split_on_pivot`]\n",
    "# 4. Split encoder inputs into:\n",
    "#       --> 'inputs' (masked sequence)\n",
    "#       --> 'labels' (unaltered sequence)\n",
    "#       --> 'weights' (sample weights; 1.0 for masked tokens and 0.0 for non-mask tokens)\n",
    "# 5. Split decoder inputs into:\n",
    "#       --> 'inputs' (unaltered sequence)\n",
    "#       --> 'labels' (sequence shifted by 1)\n",
    "\n",
    "def split_on_pivot(tokens: tf.Tensor, \n",
    "                   encoder_context_len: int = 128, \n",
    "                   decoder_context_len: int = 64, \n",
    "                   seq_len: int = 384) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\" Sample encoder and decoder input sequences from a batch of tokens with random pivot indices.\n",
    "    \n",
    "    Args:\n",
    "        tokens: A batch of token sequences with shape (batch_size, seq_len).\n",
    "        encoder_context_len: The number of tokens to be sampled for the encoder input sequences.\n",
    "        decoder_context_len: The number of tokens to be sampled for the decoder input sequences.\n",
    "        seq_len: The total length of each token sequence in the batch.\n",
    "\n",
    "    Returns:\n",
    "        encoder_inputs: A tensor with shape (batch_size, encoder_context_len) containing the\n",
    "                        sampled encoder input sequences.\n",
    "        decoder_inputs: A tensor with shape (batch_size, decoder_context_len) containing the\n",
    "                        sampled decoder input sequences.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the sum of encoder_context_len and decoder_context_len is greater than seq_len.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add one to our decoder context length as we need it for AR head\n",
    "    decoder_context_len+=1\n",
    "    \n",
    "    assert encoder_context_len + decoder_context_len <= seq_len\n",
    "    batch_size = tf.shape(tokens)[0]\n",
    "    c_point = seq_len // 2\n",
    "\n",
    "    # Sample random pivot indices for each sequence in the batch\n",
    "    pivot_indices = tf.random.uniform((batch_size, 1), minval=c_point - (c_point - encoder_context_len),\n",
    "                                      maxval=c_point + (c_point - decoder_context_len), dtype=tf.int32)\n",
    "\n",
    "    # Extract indices for examples before and after the pivot\n",
    "    indices_before = tf.range(-encoder_context_len, 0, dtype=tf.int32)\n",
    "    indices_after = tf.range(1, decoder_context_len + 1, dtype=tf.int32)\n",
    "\n",
    "    # Compute the final indices for sampling from the data\n",
    "    indices_before = tf.expand_dims(pivot_indices, 1) + indices_before\n",
    "    indices_after = tf.expand_dims(pivot_indices, 1) + indices_after\n",
    "\n",
    "    # Gather the corresponding examples from the data\n",
    "    encoder_inputs = tf.squeeze(tf.gather(tokens, indices_before, axis=1, batch_dims=1))\n",
    "    decoder_inputs = tf.squeeze(tf.gather(tokens, indices_after, axis=1, batch_dims=1))\n",
    "\n",
    "    # Reshape the encoder_inputs and decoder_inputs tensors\n",
    "    encoder_inputs = tf.reshape(encoder_inputs, (batch_size, encoder_context_len))\n",
    "    decoder_inputs = tf.reshape(decoder_inputs, (batch_size, decoder_context_len))\n",
    "\n",
    "    return tf.cast(encoder_inputs, tf.int32), tf.cast(decoder_inputs, tf.int32)\n",
    "\n",
    "def mask_sequence(sequence, vocab_size, mask_token_id, pct_to_mask=0.15, pct_to_random=0.1, pct_to_keep=0.1):\n",
    "        \"\"\" Mask a sequence of tokens. \"\"\"\n",
    "\n",
    "        # Calculate the probability of masking each token\n",
    "        masking_prob = tf.random.uniform(shape=tf.shape(sequence), minval=0, maxval=1)\n",
    "\n",
    "        # Calculate the mask based on the masking probability\n",
    "        mask = tf.cast(masking_prob < pct_to_mask, tf.int32)\n",
    "\n",
    "        # Calculate the probability of replacing with a random token\n",
    "        random_prob_mask = tf.cast(masking_prob < (pct_to_mask * pct_to_random), tf.int32)\n",
    "\n",
    "        # Calculate the probability of keeping the original token\n",
    "        keep_prob_mask = tf.cast(masking_prob < (pct_to_mask * pct_to_keep), tf.int32)\n",
    "\n",
    "        # Replace the masked tokens with the mask_token_id\n",
    "        masked_sequence = tf.where(mask == 1, mask_token_id * tf.ones_like(sequence, dtype=tf.int32), sequence)\n",
    "\n",
    "        # Replace random_prob_mask tokens with random tokens\n",
    "        random_tokens = tf.random.uniform(\n",
    "            shape=tf.shape(sequence), minval=0, maxval=vocab_size, dtype=tf.int32\n",
    "        )\n",
    "\n",
    "        # Replace the masked tokens with the mask_token_id\n",
    "        masked_sequence = tf.where(random_prob_mask == 1, random_tokens, masked_sequence)\n",
    "\n",
    "        # Keep the original tokens for keep_prob_mask\n",
    "        masked_sequence = tf.where(keep_prob_mask == 1, sequence, masked_sequence)\n",
    "\n",
    "        # Generate sample weights for masked tokens\n",
    "        sample_weights = tf.cast(mask, tf.float32)\n",
    "\n",
    "        return masked_sequence, sequence, sample_weights\n",
    "\n",
    "\n",
    "def shift_and_split_decoder_inputs(x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\" TBD \"\"\"\n",
    "    window_size = tf.shape(x)[1]-1\n",
    "    \n",
    "    # Get the indices for the first and second vectors\n",
    "    input_indices = tf.range(0, window_size, dtype=tf.int32)\n",
    "    output_indices = tf.range(1, window_size+1, dtype=tf.int32)\n",
    "\n",
    "    # Gather the corresponding columns for the first and second vectors\n",
    "    decoder_inputs = tf.gather(x, input_indices, axis=-1)\n",
    "    decoder_outputs = tf.gather(x, output_indices, axis=-1)\n",
    "\n",
    "    return decoder_inputs, decoder_outputs\n",
    "    \n",
    "def transform_sequence(sequence, vocab_size, mask_token_id):\n",
    "    encoder_inputs, decoder_inputs = split_on_pivot(sequence)\n",
    "    \n",
    "    # Encoder transform\n",
    "    encoder_inputs, encoder_labels, encoder_sample_wts = mask_sequence(\n",
    "        encoder_inputs, vocab_size, tf.constant(MASK_TOKEN_INT, dtype=tf.int32)\n",
    "    )\n",
    "    \n",
    "    # Decoder transform\n",
    "    decoder_inputs, decoder_labels = shift_and_split_decoder_inputs(decoder_inputs)\n",
    "    decoder_sample_wts = tf.ones_like(decoder_labels, dtype=tf.float32)\n",
    "    \n",
    "    _inputs = (encoder_inputs, decoder_inputs)\n",
    "    _labels = (encoder_labels, decoder_labels)\n",
    "    _sample_wts = (encoder_sample_wts, decoder_sample_wts)\n",
    "    return _inputs, _labels, _sample_wts\n",
    "    \n",
    "    \n",
    "train_ds = train_ds.shuffle(train_config[\"shuffle_buffer\"])\\\n",
    "                   .batch(train_config[\"batch_size\"], drop_remainder=True)\\\n",
    "                   .map(lambda x: transform_sequence(x, train_config[\"vocab_size\"], train_config[\"mask_token_id\"]), num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                   .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.shuffle(train_config[\"shuffle_buffer\"])\\\n",
    "               .batch(train_config[\"batch_size\"], drop_remainder=True)\\\n",
    "               .map(lambda x: transform_sequence(x, train_config[\"vocab_size\"], train_config[\"mask_token_id\"]), num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "               .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09fc0ae5-367e-4856-a691-2020ed6c2a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 17:36:08.704742: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [9]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-14 17:36:08.704968: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [9]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "_inputs, _labels, _wts = next(iter(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d68ebc-ca75-4c1d-bd54-5cdb3290fab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>span.token {font-family: Courier New; font-size: 1.1em; font-weight: 300; padding: 0px; margin-right: 0px; border-color: rgba(0, 0, 0, 0.05); border-style: ridge; border-radius: 0px;}</style><div style='background-color: #FBFBFB; line-height: 175%; padding: 25px; border-radius: 8px; margin-left: 10px; margin-right: 10px; margin-top: 20px; margin-bottom: 20px; overflow-x: auto; white-space: nowrap;'><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;fly</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ing</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>!</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;me</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;a</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;better</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;answer</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>.</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Glou</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;dear</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;lord</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;You</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;know</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;quality</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;Duke</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>,</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;How</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;un</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>rem</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>able</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;fix</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>d</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;he</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;In</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;his</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;course</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>.</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Lear</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;V</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>enge</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>ance</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>!</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;plague</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>!</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;death</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>!</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;confusion</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>!</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>ier</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>y</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;What</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;quality</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>?</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;Why</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;Gloucester</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>,</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;Gloucester</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>,</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>ld</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;speak</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;with</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;Duke</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;his</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;wife</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Glou</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Well</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>,</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;good</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;lord</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>,</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;have</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;inform</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>'</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>d</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;them</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;so</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>.</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Lear</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;In</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>form</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>'</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>d</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;them</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>?</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;thou</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>span.token {font-family: Courier New; font-size: 1.1em; font-weight: 300; padding: 0px; margin-right: 0px; border-color: rgba(0, 0, 0, 0.05); border-style: ridge; border-radius: 0px;}</style><div style='background-color: #FBFBFB; line-height: 175%; padding: 25px; border-radius: 8px; margin-left: 10px; margin-right: 10px; margin-top: 20px; margin-bottom: 20px; overflow-x: auto; white-space: nowrap;'><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;me</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>,</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;man</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>?</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Glou</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Ay</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>,</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;good</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;lord</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>.</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Lear</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;The</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;King</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;would</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;speak</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;with</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;Cornwall</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>;</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;dear</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;father</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;Would</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;with</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;his</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;daughter</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;speak</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;commands</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;her</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;service</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;Are</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;they</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;inform</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>'</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>d</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;this</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>?</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;My</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;breath</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;blood</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>!</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;F</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ier</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>y</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>?</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;fiery</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;Duke</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>span.token {font-family: Courier New; font-size: 1.1em; font-weight: 300; padding: 0px; margin-right: 0px; border-color: rgba(0, 0, 0, 0.05); border-style: ridge; border-radius: 0px;}</style><div style='background-color: #FBFBFB; line-height: 175%; padding: 25px; border-radius: 8px; margin-left: 10px; margin-right: 10px; margin-top: 20px; margin-bottom: 20px; overflow-x: auto; white-space: nowrap;'><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;fly</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ing</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;off</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>!</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;F</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>etch</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;me</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;a</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;better</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;answer</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>.</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Glou</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;My</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;dear</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;lord</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;You</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;know</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;fiery</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;quality</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;Duke</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>,</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;How</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;un</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>rem</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>ov</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>able</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;fix</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>'</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>d</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;he</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;is</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;In</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;his</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;own</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;course</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>.</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Lear</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;V</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>enge</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>ance</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>!</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;plague</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>!</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;death</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>!</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;confusion</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>!</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;F</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ier</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>y</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>?</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;What</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;quality</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>?</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;Why</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>,</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;Gloucester</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>,</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;Gloucester</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>,</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>'</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>ld</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;speak</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;with</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;Duke</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;Cornwall</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;his</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;wife</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>.</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Glou</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Well</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>,</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;good</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;lord</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>,</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;have</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;inform</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>'</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>d</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;them</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;so</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>.</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Lear</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;In</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>form</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>'</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>d</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;them</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>?</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;Dost</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;thou</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>span.token {font-family: Courier New; font-size: 1.1em; font-weight: 300; padding: 0px; margin-right: 0px; border-color: rgba(0, 0, 0, 0.05); border-style: ridge; border-radius: 0px;}</style><div style='background-color: #FBFBFB; line-height: 175%; padding: 25px; border-radius: 8px; margin-left: 10px; margin-right: 10px; margin-top: 20px; margin-bottom: 20px; overflow-x: auto; white-space: nowrap;'><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>,</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;man</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>?</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Glou</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Ay</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>,</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;good</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;lord</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>.</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Lear</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;The</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;King</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;would</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;speak</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;with</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;Cornwall</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>;</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;dear</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;father</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;Would</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;with</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;his</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;daughter</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;speak</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;commands</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;her</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;service</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;Are</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;they</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;inform</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>'</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>d</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;this</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>?</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;My</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;breath</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;blood</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>!</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;F</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ier</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>y</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>?</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;fiery</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;Duke</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>?</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'><br></span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = viz_tool(_inputs[0][0].numpy().tolist(), display_inline=True)\n",
    "_ = viz_tool(_inputs[1][0].numpy().tolist(), display_inline=True)\n",
    "\n",
    "_ = viz_tool(_labels[0][0].numpy().tolist(), display_inline=True)\n",
    "_ = viz_tool(_labels[1][0].numpy().tolist(), display_inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b7f79-3ca9-4f26-a24d-79cf1c4868b4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"font-size: 12px;\">\n",
    "<br><center><b style=\"font-size: 18px;\">TensorFloat-32 Warning:</b></center><br>This warning is related to the use of <b>TensorFloat-32</b> (<b>TF32</b>) in TensorFlow on NVIDIA Ampere architecture GPUs. <b>TensorFloat-32</b> is a new math mode in NVIDIA's A100 GPU for accelerating mixed-precision training in deep learning models. <b>TF32</b> combines the speed of lower-precision FP16 (half-precision) with the dynamic range of FP32 (single-precision).\n",
    "<br><br>\n",
    "The warning message you see is informing you that TensorFlow is using <b>TensorFloat-32</b> for matrix multiplication operations on the GPU. This is expected behavior and does not indicate a problem with your code or model. The warning message is logged only once to let you know that <b>TensorFloat-32</b> is being used for matrix multiplications.\n",
    "<br><br>\n",
    "<b>In most cases, using TensorFloat-32 can lead to significant speed improvements in training deep learning models without negatively impacting the model's accuracy or convergence.</b><br><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e31cad0-a7a4-4fc3-b9d6-e4349a6aaca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cllm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer_encoder (Transf  multiple                 2835456   \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " transformer_decoder (Transf  multiple                 3358720   \n",
      " ormerDecoder)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,194,176\n",
      "Trainable params: 6,194,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 17:36:09.722506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "from spearecode.models.cllm_backbone import CLLM\n",
    "\n",
    "# --- Model Steps ---\n",
    "# \n",
    "# 1. Define Configurations\n",
    "# 2. Load Model Architecture\n",
    "# 3. Define Optimizer and Learning Rate Details\n",
    "# 4. Define Callbacks\n",
    "#       --> TBD\n",
    "#       --> TBD\n",
    "#       --> TBD\n",
    "# 5. Define Loss Functions\n",
    "#       --> MLM Loss\n",
    "#       --> AR Loss\n",
    "# 6. Define Metrics\n",
    "#       --> TBD\n",
    "#       --> TBD\n",
    "\n",
    "enc_vocab_size, dec_vocab_size       = sp.vocab_size(), sp.vocab_size()\n",
    "enc_context_len, dec_context_len     = 128, 64\n",
    "enc_embed_dim, dec_embed_dim         = 128, 128\n",
    "enc_hidden_layers, dec_hidden_layers = 2, 2\n",
    "enc_attn_heads, dec_attn_heads       = 4, 4\n",
    "enc_ffn_act, dec_ffn_act             = \"gelu\", \"gelu\"\n",
    "enc_ffn_dropout, dec_ffn_dropout     = 0.1, 0.1\n",
    "enc_attn_dropout, dec_attn_dropout   = 0.1, 0.1\n",
    "enc_use_bias, dec_use_bias           = False, False\n",
    "enc_expansion, dec_expansion         = 4, 4\n",
    "\n",
    "enc_config = dict(\n",
    "    vocab_size=enc_vocab_size,\n",
    "    context_length=enc_context_len,\n",
    "    embedding_size=enc_embed_dim,\n",
    "    n_heads=enc_attn_heads,\n",
    "    n_layers=enc_hidden_layers,\n",
    "    use_bias=enc_use_bias,\n",
    "    ffn_act=enc_ffn_act,\n",
    "    expansion_factor=enc_expansion,\n",
    "    dropout_rate=enc_ffn_dropout,\n",
    ")\n",
    "\n",
    "dec_config = dict(\n",
    "    vocab_size=dec_vocab_size,\n",
    "    context_length=dec_context_len,\n",
    "    embedding_size=dec_embed_dim,\n",
    "    n_heads=dec_attn_heads,\n",
    "    n_layers=dec_hidden_layers,\n",
    "    use_bias=dec_use_bias,\n",
    "    ffn_act=dec_ffn_act,\n",
    "    expansion_factor=dec_expansion,\n",
    "    dropout_rate=dec_ffn_dropout,\n",
    ")\n",
    "\n",
    "cllm = CLLM(encoder_kwargs=enc_config, decoder_kwargs=dec_config, batch_size=train_config[\"batch_size\"])\n",
    "cllm.summary()\n",
    "\n",
    "# test predict\n",
    "cllm.predict(val_ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f4273ac-cfea-4d3d-9b76-a9a25591c841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spearecode.optimizers.tbd.AdamWeightDecay at 0x7f0ace043f70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spearecode.optimizers import AdamWeightDecay, WarmUpCosineDecay\n",
    "\n",
    "approx_total_steps = N_TOTAL_RECS*100\n",
    "approx_val_steps = N_VAL_RECS*100\n",
    "approx_train_steps = approx_total_steps-approx_val_steps\n",
    "\n",
    "optimizer_config = dict(\n",
    "    use_basic_adam=False,\n",
    "    use_cdecay_lr=True,\n",
    "    weight_decay_rate=0.1,\n",
    "    clipnorm=True,\n",
    "    gradient_clip_norm=1.0,\n",
    "    beta_1=1.0,\n",
    "    beta_2=0.95,\n",
    "    exclude_from_weight_decay = ['layer_normalization', 'bias'],\n",
    ")\n",
    "\n",
    "lr_config = dict(\n",
    "    init_lr=0.001,\n",
    "    min_lr=5e-05,\n",
    "    decay_portion=1.0,\n",
    "    warmup_portion=0.05,\n",
    "    hold_portion=0.01,\n",
    "    total_steps=approx_train_steps,\n",
    "    alpha=0.0,\n",
    "    decay_steps=approx_train_steps,\n",
    "    warmup_steps=int(approx_train_steps*0.05),\n",
    "    hold_steps=int(approx_train_steps*0.01),\n",
    ")\n",
    "\n",
    "# Instantiate our learning rate (or lr-schedule)\n",
    "if optimizer_config[\"use_cdecay_lr\"]:\n",
    "    optimizer_config.pop(\"use_cdecay_lr\")\n",
    "    lr=WarmUpCosineDecay(**lr_config)\n",
    "else:\n",
    "    lr=lr_config[\"init_lr\"]\n",
    "\n",
    "# Instantiate our optimizer (AdamW or just vanilla Adam)\n",
    "if not optimizer_config[\"use_basic_adam\"]:\n",
    "    optimizer_config.pop(\"use_basic_adam\")\n",
    "    optimizer = AdamWeightDecay(learning_rate=lr, **optimizer_config)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7c7ac29-d1be-4d3b-bc76-11c47adb870c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.callbacks.ModelCheckpoint at 0x7f0ace053730>,\n",
       " <keras.callbacks.EarlyStopping at 0x7f0ace053610>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spearecode.callbacks import get_callbacks\n",
    "\n",
    "CKPT_DIR = os.path.join(MODELS_DIR, \"ckpts\")\n",
    "if not os.path.isdir(CKPT_DIR): os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "cb_config = dict(\n",
    "    ckpt_dir=CKPT_DIR,\n",
    "    save_weights_only=True,\n",
    "    use_early_stopping=True,\n",
    "    es_patience=10,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "cb_list = get_callbacks(cb_config)\n",
    "cb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9388b85-771f-45f7-a5e2-1ab38ac649b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fns = [\n",
    "    tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # ENCODER MLM LOSS\n",
    "    tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # DECODER AR  LOSS\n",
    "]\n",
    "\n",
    "metrics = [\n",
    "    #TBD\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d63782f-4b38-481a-9965-ab71a7cc376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config.update({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e384fc0-1613-47f8-834b-49fcc2fd882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 18:06:26.674295: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [68]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-14 18:06:26.674524: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [68]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/losses.py\", line 2078, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/backend.py\", line 5660, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(32, 128) and logits.shape=(32, 64, 8000)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [26], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# loss_weights=[0.5, 0.5]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# metrics = TBD\u001b[39;00m\n\u001b[1;32m      3\u001b[0m cllm\u001b[38;5;241m.\u001b[39mcompile(optimizer, loss\u001b[38;5;241m=\u001b[39mloss_fns)\n\u001b[0;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mcllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file09kdk87y.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/losses.py\", line 2078, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/backend.py\", line 5660, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(32, 128) and logits.shape=(32, 64, 8000)\n"
     ]
    }
   ],
   "source": [
    "# loss_weights=[0.5, 0.5]\n",
    "# metrics = TBD\n",
    "cllm.compile(optimizer, loss=loss_fns)\n",
    "history = cllm.fit(train_ds, validation_data=val_ds, epochs=train_config[\"n_epochs\"], callbacks=cb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fad3cf84-21b6-4dc7-9ffd-3a39157d50cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(32, 128), dtype=tf.int32, name=None),\n",
       "  TensorSpec(shape=(32, 64), dtype=tf.int32, name=None)),\n",
       " (TensorSpec(shape=(32, 128), dtype=tf.int32, name=None),\n",
       "  TensorSpec(shape=(32, 64), dtype=tf.int32, name=None)),\n",
       " (TensorSpec(shape=(32, 128), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(32, 64), dtype=tf.float32, name=None)))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3251c-9a42-45a2-9044-00441c086b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
