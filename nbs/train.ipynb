{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0acb0112-c7aa-4f44-b9f2-b00adce49d8e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b>Imports and Constants</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4bf6c3-8590-4e25-95f7-24a62edd1c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 18:10:02.256938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-14 18:10:02.840087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q --upgrade tokenizer-viz\n",
    "\n",
    "# Regular imports (native python and pypi packages)\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from IPython.display import HTML, display\n",
    "from tokenizer_viz import TokenVisualization\n",
    "from tqdm.notebook import tqdm; tqdm.pandas()\n",
    "\n",
    "# Add project root into path so imports work\n",
    "PROJECT_DIR = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, PROJECT_DIR) \n",
    "\n",
    "# Our project imports\n",
    "from spearecode.utils.preprocessing_utils import (\n",
    "    load_from_txt_file, preprocess_shakespeare, save_to_txt_file, print_check_speare, get_spm_assets\n",
    ")\n",
    "from spearecode.utils.general_utils import (\n",
    "    tf_xla_jit, tf_set_memory_growth, seed_it_all, flatten_l_o_l, print_ln\n",
    ")\n",
    "from spearecode.utils.filtering_utils import (\n",
    "    save_ds_version, drop_str_from_col_names, pad_truncate_centered,\n",
    "    get_metadata_df, check_chunks, tokenize, get_n_tokens,\n",
    "    get_n_lines, get_n_chars\n",
    ")\n",
    "from spearecode.utils.tfrecord_utils import write_tfrecords, load_tfrecord_dataset\n",
    "\n",
    "TRAIN_STYLE = \"rcts_bpe_v4\"\n",
    "CHUNK_STYLE, TOK_STYLE, DS_VERSION = TRAIN_STYLE.split(\"_\")\n",
    "\n",
    "### DEFINE PATHS --- [PROJECT_DIR=\"/home/paperspace/home/spearecode\"] --- ###\n",
    "NBS_PATH = os.path.join(PROJECT_DIR, \"nbs\")\n",
    "DATA_PATH = os.path.join(PROJECT_DIR, \"data\")\n",
    "SS_TEXT_PATH = os.path.join(DATA_PATH, \"t8.shakespeare.txt\")\n",
    "PREPROCESSED_FULL_TEXT_PATH = SS_TEXT_PATH.replace(\".txt\", \"_preprocessed.txt\")\n",
    "\n",
    "DATASETS_PATH = os.path.join(DATA_PATH, \"datasets\") \n",
    "META_DIR = os.path.join(DATASETS_PATH, \"meta\") \n",
    "TFRECORD_DIR = os.path.join(DATASETS_PATH, \"tfrecords\", TRAIN_STYLE)\n",
    "MODELS_DIR = os.path.join(PROJECT_DIR, \"models\")\n",
    "\n",
    "# Specific Paths\n",
    "SPM_MODEL_PATH = os.path.join(MODELS_DIR, f\"spearecode_{TOK_STYLE}\")\n",
    "DATA_CSV_PATH  = os.path.join(DATASETS_PATH, f\"{DS_VERSION}_{CHUNK_STYLE}_{TOK_STYLE}.csv\")\n",
    "META_CSV_PATH  = os.path.join(META_DIR, f\"{DS_VERSION}_{CHUNK_STYLE}_{TOK_STYLE}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2accc846-874c-4bfc-9bef-b01ca883f5a1",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b>Instantiate expected tools for the reset of the notebook</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2dec0a8-a181-492a-ac75-f397776c522c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>token_content</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>n_lines</th>\n",
       "      <th>valid_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1\\n  From fairest creatures we desire increase...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>192</td>\n",
       "      <td>643</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2\\n  When forty winters shall besiege thy brow...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>188</td>\n",
       "      <td>662</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3\\n  Look in thy glass and tell the face thou ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>183</td>\n",
       "      <td>643</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4\\n  Unthrifty loveliness why dost thou spend,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>183</td>\n",
       "      <td>619</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5\\n  Those hours that with gentle work did fra...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>176</td>\n",
       "      <td>652</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7694</th>\n",
       "      <td>'\"Lo, this device was sent me from a nun,\\n  O...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>282</td>\n",
       "      <td>944</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7695</th>\n",
       "      <td>'\"How mighty then you are, O hear me tell!\\n  ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>298</td>\n",
       "      <td>983</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7696</th>\n",
       "      <td>'\"Now all these hearts that do on mine depend,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>283</td>\n",
       "      <td>977</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7697</th>\n",
       "      <td>'For lo, his passion, but an art of craft,\\n  ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>292</td>\n",
       "      <td>965</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7698</th>\n",
       "      <td>'Thus merely with the garment of a Grace  \\n  ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>199</td>\n",
       "      <td>630</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7699 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  \\\n",
       "0     1\\n  From fairest creatures we desire increase...   \n",
       "1     2\\n  When forty winters shall besiege thy brow...   \n",
       "2     3\\n  Look in thy glass and tell the face thou ...   \n",
       "3     4\\n  Unthrifty loveliness why dost thou spend,...   \n",
       "4     5\\n  Those hours that with gentle work did fra...   \n",
       "...                                                 ...   \n",
       "7694  '\"Lo, this device was sent me from a nun,\\n  O...   \n",
       "7695  '\"How mighty then you are, O hear me tell!\\n  ...   \n",
       "7696  '\"Now all these hearts that do on mine depend,...   \n",
       "7697  'For lo, his passion, but an art of craft,\\n  ...   \n",
       "7698  'Thus merely with the garment of a Grace  \\n  ...   \n",
       "\n",
       "                                          token_content  n_tokens  n_chars  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       192      643   \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       188      662   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       183      643   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       183      619   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       176      652   \n",
       "...                                                 ...       ...      ...   \n",
       "7694  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       282      944   \n",
       "7695  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       298      983   \n",
       "7696  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       283      977   \n",
       "7697  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       292      965   \n",
       "7698  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       199      630   \n",
       "\n",
       "      n_lines  valid_chunk  \n",
       "0          15         True  \n",
       "1          15         True  \n",
       "2          15         True  \n",
       "3          15         True  \n",
       "4          15         True  \n",
       "...       ...          ...  \n",
       "7694       23         True  \n",
       "7695       23         True  \n",
       "7696       23         True  \n",
       "7697       23         True  \n",
       "7698       15         True  \n",
       "\n",
       "[7699 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>n_lines</th>\n",
       "      <th>valid_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>192</td>\n",
       "      <td>643</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>662</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>183</td>\n",
       "      <td>643</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183</td>\n",
       "      <td>619</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>176</td>\n",
       "      <td>652</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7694</th>\n",
       "      <td>282</td>\n",
       "      <td>944</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7695</th>\n",
       "      <td>298</td>\n",
       "      <td>983</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7696</th>\n",
       "      <td>283</td>\n",
       "      <td>977</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7697</th>\n",
       "      <td>292</td>\n",
       "      <td>965</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7698</th>\n",
       "      <td>199</td>\n",
       "      <td>630</td>\n",
       "      <td>15</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7699 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      n_tokens  n_chars  n_lines  valid_chunk\n",
       "0          192      643       15         True\n",
       "1          188      662       15         True\n",
       "2          183      643       15         True\n",
       "3          183      619       15         True\n",
       "4          176      652       15         True\n",
       "...        ...      ...      ...          ...\n",
       "7694       282      944       23         True\n",
       "7695       298      983       23         True\n",
       "7696       283      977       23         True\n",
       "7697       292      965       23         True\n",
       "7698       199      630       15         True\n",
       "\n",
       "[7699 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>span.token {font-family: Courier New; font-size: 1.1em; font-weight: 300; padding: 0px; margin-right: 0px; border-color: rgba(0, 0, 0, 0.05); border-style: ridge; border-radius: 0px;}</style><div style='background-color: #FBFBFB; line-height: 175%; padding: 25px; border-radius: 8px; margin-left: 10px; margin-right: 10px; margin-top: 20px; margin-bottom: 20px; overflow-x: auto; white-space: nowrap;'><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>HERMIA</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>.</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;Yea</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>,</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;father</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>.</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>HELENA</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;And</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;H</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>i</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>pp</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>oly</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>ta</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>.</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>LYSANDER</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;And</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;he</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;did</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;bid</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;us</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;follow</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;temple</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>.</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>DEMETRIUS</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Why</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>,</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;then</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;we</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;are</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;awake</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>;</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;let</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>'</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>s</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;follow</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;him</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>And</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;by</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;way</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;let</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;us</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;rec</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>ount</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;our</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;dreams</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>.</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;Exeunt</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>BOTTOM</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;(</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>Aw</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>aking</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>)</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;When</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;c</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ue</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;comes</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>,</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;call</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;me</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;will</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;answer</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>.</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;My</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>ne</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>xt</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;is</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;'</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>Most</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;fair</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;Pyramus</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>.'</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;He</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>igh</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>-</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>ho</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>!</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;Peter</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;Qu</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>ince</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>!</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;Fl</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>ute</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>,</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>bell</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ows</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>-</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>mend</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>er</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>!</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;S</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>n</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>out</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>,</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;t</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>ink</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>er</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>!</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;St</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>ar</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ve</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>ling</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>!</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;God</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>'</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>s</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;life</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>,</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>st</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ol</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>'</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>n</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;hence</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;left</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;me</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;asleep</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>!</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;have</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;had</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;a</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;most</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;rare</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;vis</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>ion</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>.</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>I</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;have</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;had</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;a</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;dream</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;past</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;wit</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;man</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;say</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;what</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;dream</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;it</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;was</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>M</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>an</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;is</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;but</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;an</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;ass</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;if</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;he</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;go</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;about</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;exp</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>ound</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;this</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;dream</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>.</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;M</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ethought</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>I</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;was</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>-</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;there</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;is</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;no</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;man</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;can</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;tell</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;what</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>.</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;M</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>ethought</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;was</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>,</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;and</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>m</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ethought</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;had</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>,</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;but</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;man</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;is</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;but</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;a</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;p</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>atch</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>'</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>d</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;fool</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>,</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;if</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;he</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;will</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;offer</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>to</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;say</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;what</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;methought</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;had</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>.</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;The</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;eye</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;man</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;hath</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;not</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;heard</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>ear</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;man</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;hath</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;not</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;seen</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;man</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>'</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>s</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;hand</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;is</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;not</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;able</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;taste</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>,</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;his</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>tong</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ue</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;conceive</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>,</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;nor</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;his</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;heart</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;report</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>,</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;what</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;dream</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;was</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>.</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'><br></span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sp, encoder, decoder = get_spm_assets(SPM_MODEL_PATH)\n",
    "MASK_TOKEN_STR = \"[MASK]\"\n",
    "MASK_TOKEN_INT = encoder(MASK_TOKEN_STR)\n",
    "\n",
    "viz_tool = TokenVisualization(encoder, decoder, background_color=\"#FBFBFB\", transparency=0.4)\n",
    "train_df = pd.read_csv(DATA_CSV_PATH)\n",
    "meta_df  = pd.read_csv(META_CSV_PATH)\n",
    "\n",
    "display(train_df)\n",
    "display(meta_df)\n",
    "\n",
    "_ = viz_tool.visualize(train_df.content.sample(1).values[0], display_inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5570e-b19d-4de3-8014-9206f5271759",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b>Create Datasets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15f7871-e2ba-44c2-b4fb-974c7985ee3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 18:10:04.636108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 18:10:04.663823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 18:10:04.664022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 18:10:04.664962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 18:10:04.665112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 18:10:04.665229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 18:10:05.368063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 18:10:05.368255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 18:10:05.368372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-14 18:10:05.368483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13931 MB memory:  -> device: 0, name: NVIDIA RTX A4000, pci bus id: 0000:00:05.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<_MapDataset element_spec=TensorSpec(shape=(384,), dtype=tf.int64, name=None)>,\n",
       " <_MapDataset element_spec=TensorSpec(shape=(384,), dtype=tf.int64, name=None)>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all tfrecords and shuffle\n",
    "ALL_TFRECORDS = glob(os.path.join(TFRECORD_DIR, \"*.tfrec\"))\n",
    "random.shuffle(ALL_TFRECORDS)\n",
    "N_TOTAL_RECS = len(ALL_TFRECORDS)\n",
    "\n",
    "\n",
    "EX_PER_TFREC = 100\n",
    "VAL_PCT = 0.125\n",
    "N_VAL_RECS = int(VAL_PCT*N_TOTAL_RECS)\n",
    "\n",
    "VAL_TFRECORDS = ALL_TFRECORDS[:N_VAL_RECS]\n",
    "TRAIN_TFRECORDS = ALL_TFRECORDS[N_VAL_RECS:]\n",
    "\n",
    "train_ds = load_tfrecord_dataset(TRAIN_TFRECORDS)\n",
    "val_ds = load_tfrecord_dataset(VAL_TFRECORDS)\n",
    "\n",
    "(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ab3af-9ce8-444f-943d-6944a3550184",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b>Training Configuration</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71bf0125-77d4-4f2f-91be-4807a3abcf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = dict(\n",
    "    batch_size=32,\n",
    "    shuffle_buffer=512,\n",
    "    encoder_context_len=128,\n",
    "    decoder_context_len=64,\n",
    "    mask_token_id=sp.encode(\"[MASK]\")[0],\n",
    "    vocab_size=sp.vocab_size(),\n",
    "    n_epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c91c42-6f70-4430-85e8-eb7c4dbdeac8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b>TF.Data Pipeline</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddfef8be-bcba-4ee1-8208-3fda9babea73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<_PrefetchDataset element_spec=((TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), TensorSpec(shape=(32, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), TensorSpec(shape=(32, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(32, 128), dtype=tf.float32, name=None), TensorSpec(shape=(32, 64), dtype=tf.float32, name=None)))>,\n",
       " <_PrefetchDataset element_spec=((TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), TensorSpec(shape=(32, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(32, 128), dtype=tf.int32, name=None), TensorSpec(shape=(32, 64), dtype=tf.int32, name=None)), (TensorSpec(shape=(32, 128), dtype=tf.float32, name=None), TensorSpec(shape=(32, 64), dtype=tf.float32, name=None)))>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "# --- Pipeline Steps ---\n",
    "# \n",
    "# 1. Shuffle examples (shuffle_buffer)\n",
    "# 2. Batch examples (batch_size, drop_remainder, AUTOTUNE)\n",
    "# 3. Split sequence into encoder/decoder inputs [`split_on_pivot`]\n",
    "# 4. Split encoder inputs into:\n",
    "#       --> 'inputs' (masked sequence)\n",
    "#       --> 'labels' (unaltered sequence)\n",
    "#       --> 'weights' (sample weights; 1.0 for masked tokens and 0.0 for non-mask tokens)\n",
    "# 5. Split decoder inputs into:\n",
    "#       --> 'inputs' (unaltered sequence)\n",
    "#       --> 'labels' (sequence shifted by 1)\n",
    "\n",
    "def split_on_pivot(tokens: tf.Tensor, \n",
    "                   encoder_context_len: int = 128, \n",
    "                   decoder_context_len: int = 64, \n",
    "                   seq_len: int = 384) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\" Sample encoder and decoder input sequences from a batch of tokens with random pivot indices.\n",
    "    \n",
    "    Args:\n",
    "        tokens: A batch of token sequences with shape (batch_size, seq_len).\n",
    "        encoder_context_len: The number of tokens to be sampled for the encoder input sequences.\n",
    "        decoder_context_len: The number of tokens to be sampled for the decoder input sequences.\n",
    "        seq_len: The total length of each token sequence in the batch.\n",
    "\n",
    "    Returns:\n",
    "        encoder_inputs: A tensor with shape (batch_size, encoder_context_len) containing the\n",
    "                        sampled encoder input sequences.\n",
    "        decoder_inputs: A tensor with shape (batch_size, decoder_context_len) containing the\n",
    "                        sampled decoder input sequences.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the sum of encoder_context_len and decoder_context_len is greater than seq_len.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add one to our decoder context length as we need it for AR head\n",
    "    decoder_context_len+=1\n",
    "    \n",
    "    assert encoder_context_len + decoder_context_len <= seq_len\n",
    "    batch_size = tf.shape(tokens)[0]\n",
    "    c_point = seq_len // 2\n",
    "\n",
    "    # Sample random pivot indices for each sequence in the batch\n",
    "    pivot_indices = tf.random.uniform((batch_size, 1), minval=c_point - (c_point - encoder_context_len),\n",
    "                                      maxval=c_point + (c_point - decoder_context_len), dtype=tf.int32)\n",
    "\n",
    "    # Extract indices for examples before and after the pivot\n",
    "    indices_before = tf.range(-encoder_context_len, 0, dtype=tf.int32)\n",
    "    indices_after = tf.range(1, decoder_context_len + 1, dtype=tf.int32)\n",
    "\n",
    "    # Compute the final indices for sampling from the data\n",
    "    indices_before = tf.expand_dims(pivot_indices, 1) + indices_before\n",
    "    indices_after = tf.expand_dims(pivot_indices, 1) + indices_after\n",
    "\n",
    "    # Gather the corresponding examples from the data\n",
    "    encoder_inputs = tf.squeeze(tf.gather(tokens, indices_before, axis=1, batch_dims=1))\n",
    "    decoder_inputs = tf.squeeze(tf.gather(tokens, indices_after, axis=1, batch_dims=1))\n",
    "\n",
    "    # Reshape the encoder_inputs and decoder_inputs tensors\n",
    "    encoder_inputs = tf.reshape(encoder_inputs, (batch_size, encoder_context_len))\n",
    "    decoder_inputs = tf.reshape(decoder_inputs, (batch_size, decoder_context_len))\n",
    "\n",
    "    return tf.cast(encoder_inputs, tf.int32), tf.cast(decoder_inputs, tf.int32)\n",
    "\n",
    "def mask_sequence(sequence, vocab_size, mask_token_id, pct_to_mask=0.15, pct_to_random=0.1, pct_to_keep=0.1):\n",
    "        \"\"\" Mask a sequence of tokens. \"\"\"\n",
    "\n",
    "        # Calculate the probability of masking each token\n",
    "        masking_prob = tf.random.uniform(shape=tf.shape(sequence), minval=0, maxval=1)\n",
    "\n",
    "        # Calculate the mask based on the masking probability\n",
    "        mask = tf.cast(masking_prob < pct_to_mask, tf.int32)\n",
    "\n",
    "        # Calculate the probability of replacing with a random token\n",
    "        random_prob_mask = tf.cast(masking_prob < (pct_to_mask * pct_to_random), tf.int32)\n",
    "\n",
    "        # Calculate the probability of keeping the original token\n",
    "        keep_prob_mask = tf.cast(masking_prob < (pct_to_mask * pct_to_keep), tf.int32)\n",
    "\n",
    "        # Replace the masked tokens with the mask_token_id\n",
    "        masked_sequence = tf.where(mask == 1, mask_token_id * tf.ones_like(sequence, dtype=tf.int32), sequence)\n",
    "\n",
    "        # Replace random_prob_mask tokens with random tokens\n",
    "        random_tokens = tf.random.uniform(\n",
    "            shape=tf.shape(sequence), minval=0, maxval=vocab_size, dtype=tf.int32\n",
    "        )\n",
    "\n",
    "        # Replace the masked tokens with the mask_token_id\n",
    "        masked_sequence = tf.where(random_prob_mask == 1, random_tokens, masked_sequence)\n",
    "\n",
    "        # Keep the original tokens for keep_prob_mask\n",
    "        masked_sequence = tf.where(keep_prob_mask == 1, sequence, masked_sequence)\n",
    "\n",
    "        # Generate sample weights for masked tokens\n",
    "        sample_weights = tf.cast(mask, tf.float32)\n",
    "\n",
    "        return masked_sequence, sequence, sample_weights\n",
    "\n",
    "\n",
    "def shift_and_split_decoder_inputs(x: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
    "    \"\"\" TBD \"\"\"\n",
    "    window_size = tf.shape(x)[1]-1\n",
    "    \n",
    "    # Get the indices for the first and second vectors\n",
    "    input_indices = tf.range(0, window_size, dtype=tf.int32)\n",
    "    output_indices = tf.range(1, window_size+1, dtype=tf.int32)\n",
    "\n",
    "    # Gather the corresponding columns for the first and second vectors\n",
    "    decoder_inputs = tf.gather(x, input_indices, axis=-1)\n",
    "    decoder_outputs = tf.gather(x, output_indices, axis=-1)\n",
    "\n",
    "    return decoder_inputs, decoder_outputs\n",
    "    \n",
    "def transform_sequence(sequence, vocab_size, mask_token_id):\n",
    "    encoder_inputs, decoder_inputs = split_on_pivot(sequence)\n",
    "    \n",
    "    # Encoder transform\n",
    "    encoder_inputs, encoder_labels, encoder_sample_wts = mask_sequence(\n",
    "        encoder_inputs, vocab_size, tf.constant(MASK_TOKEN_INT, dtype=tf.int32)\n",
    "    )\n",
    "    \n",
    "    # Decoder transform\n",
    "    decoder_inputs, decoder_labels = shift_and_split_decoder_inputs(decoder_inputs)\n",
    "    decoder_sample_wts = tf.ones_like(decoder_labels, dtype=tf.float32)\n",
    "    \n",
    "    _inputs = (encoder_inputs, decoder_inputs)\n",
    "    _labels = (encoder_labels, decoder_labels)\n",
    "    _sample_wts = (encoder_sample_wts, decoder_sample_wts)\n",
    "    return _inputs, _labels, _sample_wts\n",
    "    \n",
    "    \n",
    "train_ds = train_ds.shuffle(train_config[\"shuffle_buffer\"])\\\n",
    "                   .batch(train_config[\"batch_size\"], drop_remainder=True)\\\n",
    "                   .map(lambda x: transform_sequence(x, train_config[\"vocab_size\"], train_config[\"mask_token_id\"]), num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "                   .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.shuffle(train_config[\"shuffle_buffer\"])\\\n",
    "               .batch(train_config[\"batch_size\"], drop_remainder=True)\\\n",
    "               .map(lambda x: transform_sequence(x, train_config[\"vocab_size\"], train_config[\"mask_token_id\"]), num_parallel_calls=tf.data.AUTOTUNE)\\\n",
    "               .prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09fc0ae5-367e-4856-a691-2020ed6c2a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 18:10:05.991892: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [9]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-14 18:10:05.992187: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [9]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    }
   ],
   "source": [
    "_inputs, _labels, _wts = next(iter(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80d68ebc-ca75-4c1d-bd54-5cdb3290fab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>span.token {font-family: Courier New; font-size: 1.1em; font-weight: 300; padding: 0px; margin-right: 0px; border-color: rgba(0, 0, 0, 0.05); border-style: ridge; border-radius: 0px;}</style><div style='background-color: #FBFBFB; line-height: 175%; padding: 25px; border-radius: 8px; margin-left: 10px; margin-right: 10px; margin-top: 20px; margin-bottom: 20px; overflow-x: auto; white-space: nowrap;'><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>AENEAS</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;(</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>Within</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;My</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>,</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;is</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;ready</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>?</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>TROILUS</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Hark</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>!</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;you</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;are</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;call</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Some</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;say</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;so</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>C</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ries</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>Come</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>'</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;him</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;that</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;must</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;die</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>.</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Bid</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;them</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;have</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;patience</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>;</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;she</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;shall</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;come</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;anon</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>PANDARUS</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Where</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;tears</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>?</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;R</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>ain</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>,</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;lay</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;this</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;wind</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;heart</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>will</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;be</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;blown</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;up</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;by</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>'</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;root</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>?</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>Exit</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>CRESSIDA</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;must</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>[MASK]</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;Gre</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>cians</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'><br></span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>span.token {font-family: Courier New; font-size: 1.1em; font-weight: 300; padding: 0px; margin-right: 0px; border-color: rgba(0, 0, 0, 0.05); border-style: ridge; border-radius: 0px;}</style><div style='background-color: #FBFBFB; line-height: 175%; padding: 25px; border-radius: 8px; margin-left: 10px; margin-right: 10px; margin-top: 20px; margin-bottom: 20px; overflow-x: auto; white-space: nowrap;'><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>TROILUS</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;No</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;remedy</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>.</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>CRESSIDA</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;A</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;woeful</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;Cressid</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;'</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>mongst</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;merry</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;Greeks</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>!</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>When</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;shall</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;we</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;see</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;again</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>?</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>TROILUS</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Hear</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;me</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>,</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;love</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>.</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;Be</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;thou</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;but</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;true</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;heart</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>-</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>CRESSIDA</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;true</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>!</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;how</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;now</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>!</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;What</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;wicked</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;de</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>em</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;is</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;this</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>?</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'><br></span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>span.token {font-family: Courier New; font-size: 1.1em; font-weight: 300; padding: 0px; margin-right: 0px; border-color: rgba(0, 0, 0, 0.05); border-style: ridge; border-radius: 0px;}</style><div style='background-color: #FBFBFB; line-height: 175%; padding: 25px; border-radius: 8px; margin-left: 10px; margin-right: 10px; margin-top: 20px; margin-bottom: 20px; overflow-x: auto; white-space: nowrap;'><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>AENEAS</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>.</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;(</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>Within</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>)</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;My</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;lord</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;is</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;lady</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;ready</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>?</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>TROILUS</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Hark</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>!</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;you</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;are</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;call</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>'</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>d</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Some</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;say</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;G</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>enius</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;so</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>C</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>ries</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;'</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>Come</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>'</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;him</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;that</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;instantly</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;must</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;die</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>.</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>Bid</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;them</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;have</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;patience</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>;</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;she</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;shall</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;come</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;anon</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>PANDARUS</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Where</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;are</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;tears</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>?</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;R</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>ain</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>,</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;lay</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;this</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;wind</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>,</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;or</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;heart</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>will</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;be</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;blown</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;up</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;by</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;th</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>'</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;root</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>?</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>Exit</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>CRESSIDA</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;must</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;then</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;to</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;Gre</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>cians</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'><br></span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>span.token {font-family: Courier New; font-size: 1.1em; font-weight: 300; padding: 0px; margin-right: 0px; border-color: rgba(0, 0, 0, 0.05); border-style: ridge; border-radius: 0px;}</style><div style='background-color: #FBFBFB; line-height: 175%; padding: 25px; border-radius: 8px; margin-left: 10px; margin-right: 10px; margin-top: 20px; margin-bottom: 20px; overflow-x: auto; white-space: nowrap;'><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>TROILUS</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;No</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;remedy</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>.</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>CRESSIDA</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;A</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;woeful</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;Cressid</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;'</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>mongst</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;the</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;merry</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;Greeks</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>!</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>When</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;shall</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;we</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;see</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;again</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>?</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>TROILUS</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;Hear</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;me</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>,</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;my</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;love</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>.</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;Be</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;thou</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;but</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;true</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;of</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;heart</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>-</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>CRESSIDA</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>.</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;I</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>&nbsp;true</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>!</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;how</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>&nbsp;now</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>!</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>&nbsp;What</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'>&nbsp;wicked</span><span class='token' style='background-color: rgba(244, 202, 228, 0.4);'>&nbsp;de</span><span class='token' style='background-color: rgba(230, 245, 201, 0.4);'>em</span><span class='token' style='background-color: rgba(255, 242, 174, 0.4);'>&nbsp;is</span><span class='token' style='background-color: rgba(241, 226, 204, 0.4);'>&nbsp;this</span><span class='token' style='background-color: rgba(204, 204, 204, 0.4);'>?</span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'><br></span><span class='token' style='background-color: rgba(179, 226, 205, 0.4);'>&nbsp;&nbsp;</span><span class='token' style='background-color: rgba(253, 205, 172, 0.4);'>TROILUS</span><span class='token' style='background-color: rgba(203, 213, 232, 0.4);'><br></span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = viz_tool(_inputs[0][0].numpy().tolist(), display_inline=True)\n",
    "_ = viz_tool(_inputs[1][0].numpy().tolist(), display_inline=True)\n",
    "\n",
    "_ = viz_tool(_labels[0][0].numpy().tolist(), display_inline=True)\n",
    "_ = viz_tool(_labels[1][0].numpy().tolist(), display_inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b7f79-3ca9-4f26-a24d-79cf1c4868b4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\" style=\"font-size: 12px;\">\n",
    "<br><center><b style=\"font-size: 18px;\">TensorFloat-32 Warning:</b></center><br>This warning is related to the use of <b>TensorFloat-32</b> (<b>TF32</b>) in TensorFlow on NVIDIA Ampere architecture GPUs. <b>TensorFloat-32</b> is a new math mode in NVIDIA's A100 GPU for accelerating mixed-precision training in deep learning models. <b>TF32</b> combines the speed of lower-precision FP16 (half-precision) with the dynamic range of FP32 (single-precision).\n",
    "<br><br>\n",
    "The warning message you see is informing you that TensorFlow is using <b>TensorFloat-32</b> for matrix multiplication operations on the GPU. This is expected behavior and does not indicate a problem with your code or model. The warning message is logged only once to let you know that <b>TensorFloat-32</b> is being used for matrix multiplications.\n",
    "<br><br>\n",
    "<b>In most cases, using TensorFloat-32 can lead to significant speed improvements in training deep learning models without negatively impacting the model's accuracy or convergence.</b><br><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e31cad0-a7a4-4fc3-b9d6-e4349a6aaca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"cllm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer_encoder (Transf  multiple                 2835456   \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " transformer_decoder (Transf  multiple                 3358720   \n",
      " ormerDecoder)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,194,176\n",
      "Trainable params: 6,194,176\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 18:10:06.984130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-04-14 18:10:07.129503: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [9]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-14 18:10:07.129780: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [9]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 894ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[ 2.33853399e-03, -1.05445618e-02,  1.57787912e-02, ...,\n",
       "          -1.89837590e-02,  1.69920251e-02,  2.07580067e-03],\n",
       "         [ 1.81162730e-02, -2.89396918e-03,  1.18544949e-02, ...,\n",
       "           7.24603003e-03, -6.87501905e-03,  2.85925041e-03],\n",
       "         [ 2.03935243e-03, -1.11114727e-02,  2.24376712e-02, ...,\n",
       "          -7.11629540e-03, -5.36899548e-03, -1.48997931e-02],\n",
       "         ...,\n",
       "         [ 3.96782812e-03, -2.57622148e-03,  1.71221849e-02, ...,\n",
       "          -1.81406215e-02,  5.58015425e-03,  1.77714471e-02],\n",
       "         [ 7.10261520e-03, -4.50161286e-03,  1.51792038e-02, ...,\n",
       "          -1.91542562e-02,  9.30690207e-03,  1.23434113e-02],\n",
       "         [ 8.44348315e-03,  1.58557901e-03,  2.57380493e-02, ...,\n",
       "          -1.00576598e-02,  1.01305009e-03,  2.15468369e-02]],\n",
       " \n",
       "        [[ 8.41643475e-03, -1.52913788e-02,  1.05650434e-02, ...,\n",
       "          -2.31477879e-02,  1.35068120e-02, -9.32093151e-03],\n",
       "         [ 1.14050889e-02, -9.51392762e-03,  1.29568614e-02, ...,\n",
       "          -1.71991643e-02,  5.70376683e-03, -1.13944141e-02],\n",
       "         [ 1.17890043e-02, -2.61412258e-03,  1.56647805e-02, ...,\n",
       "          -1.50138577e-02,  2.99714296e-03, -1.49756251e-02],\n",
       "         ...,\n",
       "         [ 1.03721321e-02, -2.52188719e-03,  1.71746984e-02, ...,\n",
       "          -2.07235273e-02,  1.05297128e-02,  1.35636963e-02],\n",
       "         [ 5.70953777e-03, -3.80529254e-03,  2.17255428e-02, ...,\n",
       "          -5.46771521e-03,  3.74353840e-04,  9.77981556e-03],\n",
       "         [-2.05231248e-04, -4.61411243e-03,  2.20328718e-02, ...,\n",
       "          -5.86042972e-03, -8.65321420e-03,  2.15239413e-02]],\n",
       " \n",
       "        [[-3.10548116e-03, -7.15417182e-03,  7.28979940e-04, ...,\n",
       "          -1.08263185e-02,  4.01770137e-03, -2.21208250e-03],\n",
       "         [ 3.23913433e-03, -1.73081663e-02,  4.70844284e-03, ...,\n",
       "          -2.36271764e-03, -1.28541235e-02, -7.05987797e-04],\n",
       "         [ 1.21800462e-02,  2.02464499e-03,  2.91428287e-02, ...,\n",
       "          -1.17297797e-03, -3.51545541e-03, -2.60063284e-03],\n",
       "         ...,\n",
       "         [ 1.00480337e-02, -3.57270474e-04,  1.63129158e-02, ...,\n",
       "          -1.22780064e-02, -9.17785708e-03,  1.32893287e-02],\n",
       "         [ 8.34746193e-03, -1.30120770e-03,  2.22208947e-02, ...,\n",
       "          -1.33589283e-02, -7.38687254e-03,  2.10427754e-02],\n",
       "         [ 1.13803631e-04, -2.54722731e-03,  1.46415969e-02, ...,\n",
       "          -1.03086904e-02, -8.23816098e-03,  9.77775455e-03]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 8.48767720e-03, -1.52741680e-02,  1.06554795e-02, ...,\n",
       "          -2.32614595e-02,  1.34496801e-02, -8.94606020e-03],\n",
       "         [ 1.15288571e-02, -9.49607976e-03,  1.31163113e-02, ...,\n",
       "          -1.72548406e-02,  5.59245748e-03, -1.11199971e-02],\n",
       "         [ 1.19608585e-02, -2.66212574e-03,  1.59234107e-02, ...,\n",
       "          -1.50691783e-02,  2.81364121e-03, -1.47337820e-02],\n",
       "         ...,\n",
       "         [ 1.04363980e-02, -2.88094510e-03,  1.69696975e-02, ...,\n",
       "          -2.08075065e-02,  1.07810451e-02,  1.40948668e-02],\n",
       "         [ 5.78380004e-03, -4.18304140e-03,  2.16380674e-02, ...,\n",
       "          -5.52272936e-03,  4.91219340e-04,  1.02941263e-02],\n",
       "         [-1.60284201e-03, -4.02075518e-03,  2.16704216e-02, ...,\n",
       "          -7.09457230e-03,  4.30347025e-03,  8.86763167e-03]],\n",
       " \n",
       "        [[ 1.20732225e-02, -1.13220410e-02,  1.95259806e-02, ...,\n",
       "           4.85382043e-05,  1.17994426e-02, -2.05938262e-03],\n",
       "         [ 2.55967909e-03, -1.10604139e-02,  1.25995595e-02, ...,\n",
       "          -1.26076899e-02,  6.63422421e-03, -7.14929309e-03],\n",
       "         [ 6.99238339e-03, -1.56085007e-02,  1.33391740e-02, ...,\n",
       "           3.12088546e-03, -8.20219051e-03, -1.34827467e-02],\n",
       "         ...,\n",
       "         [ 1.42576937e-02,  1.44283113e-04,  2.34014224e-02, ...,\n",
       "          -6.59488793e-03, -1.22953653e-02,  2.59365160e-02],\n",
       "         [ 1.04755266e-02,  4.78662411e-03,  2.57550236e-02, ...,\n",
       "          -8.61644186e-03, -6.43903913e-04,  3.11832298e-02],\n",
       "         [ 4.28587548e-04, -2.07860116e-03,  2.41235532e-02, ...,\n",
       "          -9.05623659e-03, -1.07349386e-03,  2.09739842e-02]],\n",
       " \n",
       "        [[ 8.38506687e-03, -1.52886454e-02,  1.05710654e-02, ...,\n",
       "          -2.31045410e-02,  1.35716395e-02, -9.22210980e-03],\n",
       "         [ 1.13582239e-02, -9.50228609e-03,  1.29578914e-02, ...,\n",
       "          -1.71536785e-02,  5.74949058e-03, -1.12999268e-02],\n",
       "         [ 1.17472773e-02, -2.63954070e-03,  1.57065559e-02, ...,\n",
       "          -1.49866836e-02,  3.03818332e-03, -1.48840314e-02],\n",
       "         ...,\n",
       "         [ 9.21884365e-03, -2.52240384e-03,  2.32262667e-02, ...,\n",
       "          -8.46005417e-03,  3.13231326e-03,  1.29592251e-02],\n",
       "         [ 6.22028811e-03,  1.31726870e-03,  1.94669385e-02, ...,\n",
       "          -8.25437997e-03, -2.68206396e-03,  2.17042081e-02],\n",
       "         [-3.55796074e-04, -3.88669735e-03,  1.58956163e-02, ...,\n",
       "          -1.91836078e-02,  1.29148178e-02,  1.08113708e-02]]],\n",
       "       dtype=float32),\n",
       " array([[[-1.64194236e-04,  6.11600408e-04,  9.76063457e-05, ...,\n",
       "          -1.14954317e-04,  1.03386189e-03,  1.21091784e-04],\n",
       "         [ 1.91203231e-04,  9.35189018e-04,  2.82337947e-04, ...,\n",
       "           1.32827976e-04,  1.14494935e-03,  2.61222624e-04],\n",
       "         [ 3.36019642e-04,  1.02098659e-03,  2.78751453e-04, ...,\n",
       "          -2.17739376e-04,  1.09915121e-03, -2.05197823e-04],\n",
       "         ...,\n",
       "         [-1.51610031e-04,  3.50210932e-04, -1.87480429e-04, ...,\n",
       "           1.29482738e-04,  3.59447324e-04, -1.61292759e-04],\n",
       "         [-1.94030581e-04,  2.17099645e-04, -9.44050335e-05, ...,\n",
       "           8.08743134e-05,  5.24174247e-04, -6.27826812e-05],\n",
       "         [-9.23262123e-05,  1.46090606e-04,  4.51568449e-05, ...,\n",
       "           6.48054993e-06,  6.80808444e-04, -1.93098585e-05]],\n",
       " \n",
       "        [[ 3.34975019e-04,  1.16760808e-03,  3.17246042e-04, ...,\n",
       "          -3.60609061e-04,  8.96166428e-04, -1.52187058e-05],\n",
       "         [ 4.05203929e-04,  8.29234836e-04, -6.46452754e-05, ...,\n",
       "          -1.86689795e-04,  7.40319199e-04, -2.61527457e-05],\n",
       "         [ 3.99563927e-04,  1.16560271e-03, -2.79685628e-06, ...,\n",
       "          -4.24223217e-05,  6.71111164e-04,  1.27576597e-04],\n",
       "         ...,\n",
       "         [-1.20096782e-04, -5.22896480e-05, -1.88351594e-04, ...,\n",
       "          -2.07106743e-04,  1.06272448e-04, -5.27195691e-04],\n",
       "         [ 1.85221026e-04,  1.94578577e-04, -1.02274505e-04, ...,\n",
       "           8.51999866e-05,  6.58014615e-04, -2.16381508e-04],\n",
       "         [ 3.87068692e-04,  3.09243827e-04,  4.04849183e-04, ...,\n",
       "          -4.20220094e-05,  2.98127183e-04, -2.60578498e-04]],\n",
       " \n",
       "        [[ 6.58686855e-04,  8.73771904e-04,  1.90587438e-04, ...,\n",
       "          -6.60294463e-05,  9.20859224e-04, -6.13352895e-05],\n",
       "         [ 5.86494207e-05,  6.36212120e-04,  2.06259836e-04, ...,\n",
       "          -2.57537002e-04,  1.11841853e-03, -7.44145800e-05],\n",
       "         [ 2.74915394e-04,  9.99225886e-04,  2.57755106e-04, ...,\n",
       "          -1.30418397e-04,  6.07638853e-04, -2.37166518e-04],\n",
       "         ...,\n",
       "         [-1.47505561e-04,  3.48658155e-04, -1.93813583e-04, ...,\n",
       "           1.26174709e-04,  3.63270607e-04, -1.54406996e-04],\n",
       "         [-1.89900529e-04,  2.15611522e-04, -1.00119927e-04, ...,\n",
       "           7.73439824e-05,  5.27976488e-04, -5.61457528e-05],\n",
       "         [-8.80346633e-05,  1.44759397e-04,  3.93691444e-05, ...,\n",
       "           3.75488526e-06,  6.84361206e-04, -1.24085527e-05]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-2.02963740e-04,  8.42717069e-04,  6.86352840e-04, ...,\n",
       "          -2.51215315e-05,  1.06194522e-03, -1.54964815e-04],\n",
       "         [ 3.18665116e-06,  8.82985129e-04,  6.64586085e-04, ...,\n",
       "          -9.58112360e-05,  1.23420253e-03, -1.30243090e-04],\n",
       "         [ 3.62061372e-04,  8.81173124e-04, -1.27077845e-04, ...,\n",
       "          -2.04528493e-04,  5.70619886e-04, -1.21715857e-05],\n",
       "         ...,\n",
       "         [-1.38828225e-04,  3.54021904e-04, -1.88645063e-04, ...,\n",
       "           1.34904200e-04,  3.34522076e-04, -1.50021311e-04],\n",
       "         [-1.82342381e-04,  2.19967798e-04, -9.53346971e-05, ...,\n",
       "           8.63999303e-05,  4.98770154e-04, -5.19942441e-05],\n",
       "         [-8.04694864e-05,  1.48463077e-04,  4.32051274e-05, ...,\n",
       "           1.27595558e-05,  6.54625706e-04, -8.95412813e-06]],\n",
       " \n",
       "        [[ 2.23846029e-04,  9.58900724e-04,  2.42740061e-04, ...,\n",
       "          -3.21659114e-04,  7.59687333e-04, -6.27829053e-04],\n",
       "         [ 3.12986813e-04,  9.94605361e-04,  2.71985977e-04, ...,\n",
       "          -2.52812042e-05,  6.66895648e-04,  6.49850845e-05],\n",
       "         [ 5.25017560e-04,  7.71861814e-04, -3.68851848e-04, ...,\n",
       "          -4.80189279e-04,  5.85629954e-04,  5.97344660e-05],\n",
       "         ...,\n",
       "         [-3.04528985e-05,  5.50460245e-04, -7.18462034e-05, ...,\n",
       "           1.93679967e-04,  6.76319527e-04, -2.53821752e-04],\n",
       "         [ 3.55442811e-04,  5.54784565e-05, -3.73290124e-04, ...,\n",
       "          -3.33550444e-04,  2.40542693e-04, -7.56759546e-04],\n",
       "         [ 3.00342381e-05, -6.14637829e-05,  2.86273949e-04, ...,\n",
       "          -1.03706836e-04,  7.28896877e-04, -4.47110906e-05]],\n",
       " \n",
       "        [[ 2.39986126e-04,  1.16909400e-03,  3.75651842e-04, ...,\n",
       "           1.67442980e-04,  1.02412375e-03,  7.89034166e-05],\n",
       "         [ 4.00427816e-04,  1.28173782e-03,  3.27411981e-04, ...,\n",
       "          -2.81502173e-04,  8.70278745e-04, -1.32481247e-04],\n",
       "         [ 2.03472751e-04,  6.09688752e-04,  2.06957920e-05, ...,\n",
       "          -5.59161999e-05,  8.08480778e-04, -2.17383727e-04],\n",
       "         ...,\n",
       "         [ 1.41185345e-04,  1.99277099e-04, -1.75740468e-04, ...,\n",
       "          -7.53768691e-05,  6.46009808e-04, -8.20639485e-04],\n",
       "         [-6.38403289e-05, -1.07366468e-05,  1.66329250e-04, ...,\n",
       "          -3.66680688e-05,  5.95234975e-04, -1.02693128e-04],\n",
       "         [ 1.99494301e-04,  1.84622513e-06,  7.31510445e-05, ...,\n",
       "          -4.25414386e-04,  8.70246382e-04, -7.87689933e-04]]],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spearecode.models.cllm_backbone import CLLM\n",
    "\n",
    "# --- Model Steps ---\n",
    "# \n",
    "# 1. Define Configurations\n",
    "# 2. Load Model Architecture\n",
    "# 3. Define Optimizer and Learning Rate Details\n",
    "# 4. Define Callbacks\n",
    "#       --> TBD\n",
    "#       --> TBD\n",
    "#       --> TBD\n",
    "# 5. Define Loss Functions\n",
    "#       --> MLM Loss\n",
    "#       --> AR Loss\n",
    "# 6. Define Metrics\n",
    "#       --> TBD\n",
    "#       --> TBD\n",
    "\n",
    "enc_vocab_size, dec_vocab_size       = sp.vocab_size(), sp.vocab_size()\n",
    "enc_context_len, dec_context_len     = 128, 64\n",
    "enc_embed_dim, dec_embed_dim         = 128, 128\n",
    "enc_hidden_layers, dec_hidden_layers = 2, 2\n",
    "enc_attn_heads, dec_attn_heads       = 4, 4\n",
    "enc_ffn_act, dec_ffn_act             = \"gelu\", \"gelu\"\n",
    "enc_ffn_dropout, dec_ffn_dropout     = 0.1, 0.1\n",
    "enc_attn_dropout, dec_attn_dropout   = 0.1, 0.1\n",
    "enc_use_bias, dec_use_bias           = False, False\n",
    "enc_expansion, dec_expansion         = 4, 4\n",
    "\n",
    "enc_config = dict(\n",
    "    vocab_size=enc_vocab_size,\n",
    "    context_length=enc_context_len,\n",
    "    embedding_size=enc_embed_dim,\n",
    "    n_heads=enc_attn_heads,\n",
    "    n_layers=enc_hidden_layers,\n",
    "    use_bias=enc_use_bias,\n",
    "    ffn_act=enc_ffn_act,\n",
    "    expansion_factor=enc_expansion,\n",
    "    dropout_rate=enc_ffn_dropout,\n",
    ")\n",
    "\n",
    "dec_config = dict(\n",
    "    vocab_size=dec_vocab_size,\n",
    "    context_length=dec_context_len,\n",
    "    embedding_size=dec_embed_dim,\n",
    "    n_heads=dec_attn_heads,\n",
    "    n_layers=dec_hidden_layers,\n",
    "    use_bias=dec_use_bias,\n",
    "    ffn_act=dec_ffn_act,\n",
    "    expansion_factor=dec_expansion,\n",
    "    dropout_rate=dec_ffn_dropout,\n",
    ")\n",
    "\n",
    "cllm = CLLM(encoder_kwargs=enc_config, decoder_kwargs=dec_config, batch_size=train_config[\"batch_size\"])\n",
    "cllm.summary()\n",
    "\n",
    "# test predict\n",
    "cllm.predict(val_ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f4273ac-cfea-4d3d-9b76-a9a25591c841",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.optimizers.adam.Adam at 0x7f9c364a4d90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spearecode.optimizers import AdamWeightDecay, WarmUpCosineDecay\n",
    "\n",
    "approx_total_steps = N_TOTAL_RECS*100\n",
    "approx_val_steps = N_VAL_RECS*100\n",
    "approx_train_steps = approx_total_steps-approx_val_steps\n",
    "\n",
    "optimizer_config = dict(\n",
    "    use_basic_adam=True,\n",
    "    use_cdecay_lr=True,\n",
    "    weight_decay_rate=0.1,\n",
    "    clipnorm=True,\n",
    "    gradient_clip_norm=1.0,\n",
    "    beta_1=1.0,\n",
    "    beta_2=0.95,\n",
    "    exclude_from_weight_decay = ['layer_normalization', 'bias'],\n",
    ")\n",
    "\n",
    "lr_config = dict(\n",
    "    init_lr=0.001,\n",
    "    min_lr=5e-05,\n",
    "    decay_portion=1.0,\n",
    "    warmup_portion=0.05,\n",
    "    hold_portion=0.01,\n",
    "    total_steps=approx_train_steps,\n",
    "    alpha=0.0,\n",
    "    decay_steps=approx_train_steps,\n",
    "    warmup_steps=int(approx_train_steps*0.05),\n",
    "    hold_steps=int(approx_train_steps*0.01),\n",
    ")\n",
    "\n",
    "# Instantiate our learning rate (or lr-schedule)\n",
    "if optimizer_config[\"use_cdecay_lr\"]:\n",
    "    optimizer_config.pop(\"use_cdecay_lr\")\n",
    "    lr=WarmUpCosineDecay(**lr_config)\n",
    "else:\n",
    "    lr=lr_config[\"init_lr\"]\n",
    "\n",
    "# Instantiate our optimizer (AdamW or just vanilla Adam)\n",
    "if not optimizer_config[\"use_basic_adam\"]:\n",
    "    optimizer_config.pop(\"use_basic_adam\")\n",
    "    optimizer = AdamWeightDecay(learning_rate=lr, **optimizer_config)\n",
    "else:\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7c7ac29-d1be-4d3b-bc76-11c47adb870c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.callbacks.ModelCheckpoint at 0x7f9c2c142430>,\n",
       " <keras.callbacks.EarlyStopping at 0x7f9c2c1427f0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spearecode.callbacks import get_callbacks\n",
    "\n",
    "CKPT_DIR = os.path.join(MODELS_DIR, \"ckpts\")\n",
    "if not os.path.isdir(CKPT_DIR): os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "cb_config = dict(\n",
    "    ckpt_dir=CKPT_DIR,\n",
    "    save_weights_only=True,\n",
    "    use_early_stopping=True,\n",
    "    es_patience=10,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "cb_list = get_callbacks(cb_config)\n",
    "cb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9388b85-771f-45f7-a5e2-1ab38ac649b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fns = [\n",
    "    tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # ENCODER MLM LOSS\n",
    "    tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), # DECODER AR  LOSS\n",
    "]\n",
    "\n",
    "metrics = [\n",
    "    #TBD\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e384fc0-1613-47f8-834b-49fcc2fd882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 18:14:49.408932: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x5ab60d10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-14 18:14:49.408951: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA RTX A4000, Compute Capability 8.6\n",
      "2023-04-14 18:14:49.412920: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-14 18:14:49.686645: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:49.710886: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:49.711246: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:49.711325: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "\t [[{{node StatefulPartitionedCall_21}}]]\n",
      "2023-04-14 18:14:49.753463: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:49.778544: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:49.778932: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:49.792832: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:49.817554: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:49.818099: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:49.831873: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:49.856045: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:49.856466: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:49.870018: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:49.894692: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:49.895062: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:49.909194: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:49.933568: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:49.933938: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:49.947226: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:49.971023: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:49.971369: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:49.984267: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.008316: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.008698: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.021872: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.045658: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.046028: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.059298: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.083115: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.083474: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.096733: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.120748: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.121087: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.134242: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.158225: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.158603: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.171703: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.195631: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.196021: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.209186: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.232950: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.233288: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.246270: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.270188: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.270566: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.283859: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.308161: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.308540: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.321934: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.345916: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.346292: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.359594: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.383450: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.383834: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.397627: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.422219: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.422609: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.435993: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.459817: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.460207: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.473172: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.497046: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.497449: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.510940: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.534822: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.535166: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.548323: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.572256: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.572646: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.585764: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.610047: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.610441: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.623383: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.647430: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.647842: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.661239: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.685128: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.685509: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.699134: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.723020: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.723471: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.736710: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.760502: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.760873: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.774398: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.798340: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.798727: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.812199: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.836031: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.836421: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.849566: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.873620: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.874038: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.887210: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.911066: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.911455: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.924527: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.948396: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.948779: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.962015: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:50.985950: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:50.986343: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:50.999741: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:51.023670: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:51.024087: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:51.037360: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:51.061148: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:51.061537: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:51.074706: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:51.098643: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:51.099054: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:51.112598: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:51.136401: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:51.136790: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:51.150415: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:51.174332: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:51.174731: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:51.188154: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:51.211970: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:51.212345: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:51.225761: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:51.250471: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:51.250832: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:51.263873: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:51.287697: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:51.288089: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:51.301311: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:51.325372: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:51.325804: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "2023-04-14 18:14:51.339786: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:417] Loaded runtime CuDNN library: 8.5.0 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-04-14 18:14:51.364088: E tensorflow/compiler/xla/status_macros.cc:57] INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n",
      "*** Begin stack trace ***\n",
      "\ttsl::CurrentStackTrace[abi:cxx11]()\n",
      "\t\n",
      "\txla::status_macros::MakeErrorStream::Impl::GetStatus()\n",
      "\txla::gpu::GpuCompiler::OptimizeHloModule(xla::HloModule*, stream_executor::StreamExecutor*, stream_executor::DeviceMemoryAllocator*, xla::gpu::GpuTargetConfig const&, xla::AutotuneResults const*)\n",
      "\txla::gpu::GpuCompiler::RunHloPasses(std::unique_ptr<xla::HloModule, std::default_delete<xla::HloModule> >, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&)\n",
      "\txla::Service::BuildExecutable(xla::HloModuleProto const&, std::unique_ptr<xla::HloModuleConfig, std::default_delete<xla::HloModuleConfig> >, xla::Backend*, stream_executor::StreamExecutor*, xla::Compiler::CompileOptions const&, bool)\n",
      "\txla::LocalService::CompileExecutables(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\txla::LocalClient::Compile(xla::XlaComputation const&, absl::lts_20220623::Span<xla::Shape const* const>, xla::ExecutableBuildOptions const&)\n",
      "\ttensorflow::XlaDeviceCompilerClient::BuildExecutable(tensorflow::XlaCompiler::Options const&, tensorflow::XlaCompilationResult const&)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileStrict(tensorflow::DeviceCompilationClusterSignature const&, tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::NameAttrList const&, tensorflow::DeviceCompilationCache<xla::LocalExecutable>::Value, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tsl::mutex*)\n",
      "\ttensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileImpl(tensorflow::XlaCompiler::CompileOptions const&, tensorflow::XlaCompiler::Options const&, tensorflow::NameAttrList const&, std::vector<tensorflow::XlaArgument, std::allocator<tensorflow::XlaArgument> > const&, tensorflow::DeviceCompiler<xla::LocalExecutable, xla::LocalClient>::CompileScope, tensorflow::DeviceCompileMode, tensorflow::OpKernelContext*, tensorflow::DeviceCompilationProfiler*, tensorflow::XlaCompilationResult const**, xla::LocalExecutable**)\n",
      "\t\n",
      "\ttensorflow::XlaLocalLaunchBase::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\ttensorflow::BaseGPUDevice::ComputeAsync(tensorflow::AsyncOpKernel*, tensorflow::OpKernelContext*, std::function<void ()>)\n",
      "\t\n",
      "\t\n",
      "\tEigen::ThreadPoolTempl<tsl::thread::EigenEnvironment>::WorkerLoop(int)\n",
      "\tstd::_Function_handler<void (), tsl::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)\n",
      "\t\n",
      "\t\n",
      "\tclone\n",
      "*** End stack trace ***\n",
      "\n",
      "2023-04-14 18:14:51.364533: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:362 : INTERNAL: RET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_21' defined at (most recent call last):\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/traitlets/config/application.py\", line 1041, in launch_instance\n      app.start()\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_8549/1517078499.py\", line 4, in <module>\n      history = cllm.fit(train_ds, validation_data=val_ds, epochs=train_config[\"n_epochs\"], callbacks=cb_list)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1245, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_21'\nRET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n\t [[{{node StatefulPartitionedCall_21}}]] [Op:__inference_train_function_16869]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# loss_weights=[0.5, 0.5]\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# metrics = TBD\u001b[39;00m\n\u001b[1;32m      3\u001b[0m cllm\u001b[38;5;241m.\u001b[39mcompile(optimizer, loss\u001b[38;5;241m=\u001b[39mloss_fns)\n\u001b[0;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mcllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn_epochs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'StatefulPartitionedCall_21' defined at (most recent call last):\n    File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/traitlets/config/application.py\", line 1041, in launch_instance\n      app.start()\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2885, in run_cell\n      result = self._run_cell(\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3139, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3318, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_8549/1517078499.py\", line 4, in <module>\n      history = cllm.fit(train_ds, validation_data=val_ds, epochs=train_config[\"n_epochs\"], callbacks=cb_list)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/engine/training.py\", line 1054, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 543, in minimize\n      self.apply_gradients(grads_and_vars)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1174, in apply_gradients\n      return super().apply_gradients(grads_and_vars, name=name)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 650, in apply_gradients\n      iteration = self._internal_apply_gradients(grads_and_vars)\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1200, in _internal_apply_gradients\n      return tf.__internal__.distribute.interim.maybe_merge_call(\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1250, in _distributed_apply_gradients_fn\n      distribution.extended.update(\n    File \"/home/paperspace/.local/lib/python3.9/site-packages/keras/optimizers/optimizer.py\", line 1245, in apply_grad_to_update_var\n      return self._update_step_xla(grad, var, id(self._var_key(var)))\nNode: 'StatefulPartitionedCall_21'\nRET_CHECK failure (tensorflow/compiler/xla/service/gpu/gpu_compiler.cc:618) dnn != nullptr \n\t [[{{node StatefulPartitionedCall_21}}]] [Op:__inference_train_function_16869]"
     ]
    }
   ],
   "source": [
    "# loss_weights=[0.5, 0.5]\n",
    "# metrics = TBD\n",
    "cllm.compile(optimizer, loss=loss_fns)\n",
    "history = cllm.fit(train_ds, validation_data=val_ds, epochs=train_config[\"n_epochs\"], callbacks=cb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baa3251c-9a42-45a2-9044-00441c086b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,s = next(iter(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b11f142-2f49-4203-9680-d434e0e2e6ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 128), dtype=int32, numpy=\n",
       " array([[   0,    0,    0, ...,  549, 1124, 2804],\n",
       "        [7938,   92,    9, ..., 7938,   48,   83],\n",
       "        [   9,  460, 2031, ...,  707, 3554, 7938],\n",
       "        ...,\n",
       "        [ 155, 4992,   29, ..., 7938,   44, 1248],\n",
       "        [   0,    0,    0, ..., 1102, 1255,    4],\n",
       "        [5005, 2379, 7970, ..., 7953, 7930, 4513]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(32, 64), dtype=int32, numpy=\n",
       " array([[  72,   46, 1895, ..., 1204,   65, 4443],\n",
       "        [   4, 7977, 2402, ...,    0,    0,    0],\n",
       "        [7483,  353,  105, ..., 7981,    4,   11],\n",
       "        ...,\n",
       "        [1351,  117, 7969, ..., 2633, 7953, 7934],\n",
       "        [ 198, 1502,  275, ..., 7939,    4,   11],\n",
       "        [   4,   11, 3593, ...,    4,   11, 1121]], dtype=int32)>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86558b-c39e-44a7-bad2-4d71fdcd22fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
