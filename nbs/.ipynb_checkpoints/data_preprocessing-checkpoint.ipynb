{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29279551-2a6d-480f-8fe2-d18017f56b20",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# üöÄ Spearecode Preprocessing üöÄ\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Welcome to the **Spearecode Preprocessing Notebook**! This notebook will guide you through the necessary preprocessing steps to prepare a toy dataset for Language Model training. We will focus on making the dataset more suitable for training by performing the following steps:\n",
    "\n",
    "1. üìö **Loading the dataset**: We'll start by importing the dataset from a file or external source.\n",
    "2. üì¶ **Chunking the text**: The dataset will be divided into smaller chunks or segments, making it easier to process during training.\n",
    "3. üí¨ **Tokenization**: Each chunk of text will be split into individual tokens (words or subwords), which are the basic units for language models.\n",
    "4. üìä **Basic Exploratory Data Analysis (EDA)**: We'll analyze the dataset's characteristics, such as token frequency, to gain insights and identify potential issues.\n",
    "\n",
    "After completing the preprocessing and EDA, the toy dataset will be converted into `TFRecords` format. This efficient binary format is designed for use with TensorFlow and will enable seamless integration with your Language Model training pipeline.\n",
    "\n",
    "Let's dive in and start preprocessing the dataset! üéâ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a62943-0d3c-4aae-933b-2a5fbe751ad6",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "# üåü Table of Contents üåü\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "0. [**Setup**](#setup)\n",
    "1. [**Loading the Dataset**](#loading-the-dataset)\n",
    "2. [**Chunking the Text**](#chunking-the-text)\n",
    "3. [**Tokenization**](#tokenization)\n",
    "4. [**Basic Exploratory Data Analysis (EDA)**](#basic-eda)\n",
    "5. [**Converting to TFRecords**](#converting-to-tfrecords)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e83729-e61a-4d94-a074-17ccb60593dc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üõ†Ô∏è Setup <a name=\"setup\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this section, we'll import required libraries and methods from our utilities file. We will also define relevant paths and high level information we may need later. We also run a few basic Tensorflow setup steps to ensure optimal and reproducible runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab4c015-345e-4c48-a749-c7094604fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "PROJECT_DIR = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, PROJECT_DIR) # project root into path\n",
    "\n",
    "from spearecode.preprocessing_utils import load_from_txt_file, preprocess_shakespeare, save_to_txt_file, print_check_speare\n",
    "from spearecode.general_utils import tf_xla_jit, tf_set_memory_growth, seed_it_all, flatten_l_o_l, print_ln\n",
    "\n",
    "### DEFINE PATHS ###\n",
    "DATA_PATH = os.path.join(PROJECT_DIR, \"data\")\n",
    "SS_TEXT_PATH = os.path.join(DATA_PATH, \"t8.shakespeare.txt\")\n",
    "NBS_PATH = os.path.join(PROJECT_DIR, \"nbs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975cca8-555a-4871-a41a-3ba37c72aa12",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üìö Loading the Dataset <a name=\"loading-the-dataset\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this section, we'll import the dataset from a file or external source. The dataset will be read into memory, allowing us to manipulate and process the text as needed throughout the preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7fa905-5791-48de-9933-bcd0fcfe95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_FULL_TEXT_PATH = SS_TEXT_PATH.replace(\".txt\", \"_preprocessed.txt\")\n",
    "raw_text = load_from_txt_file(SS_TEXT_PATH)\n",
    "ss_text = preprocess_shakespeare(raw_text)\n",
    "save_to_txt_file(ss_text, PREPROCESSED_FULL_TEXT_PATH)\n",
    "print_check_speare(ss_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ead5a1-5ce2-448f-83fd-7c38cd7ba060",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üì¶ Chunking the Text <a name=\"chunking-the-text\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Once the dataset is loaded, we'll divide it into smaller chunks or segments. This step is crucial for making the dataset more similar to code files (which is the type of data we will be using during the other parallel streams).\n",
    "\n",
    "I implement two simple methods:\n",
    "1. A basic double newline split **(`\\n\\n`)** resulting in 6294 chunks\n",
    "2. Using Langchain RecursiveTextSplitter to chunk to a particular text length\n",
    "    * This allows us to specify our desired text length and even overlap the chunks.\n",
    "        * Note we allow for a small amount of overlap and this may cause some leakage... but whatever.\n",
    "    * **We will use this method for our purposes.**\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8bb3c7-0144-4bb9-ba56-97e91893fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_rcts_chunking(text, chunk_size=512, chunk_overlap=64, length_fn=len):\n",
    "    \"\"\"\n",
    "    Perform Recursive Character Text Splitting (RCTS) chunking on the input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be chunked.\n",
    "        chunk_size (int): The maximum size of each chunk.\n",
    "        chunk_overlap (int): The number of overlapping characters between adjacent chunks.\n",
    "        length_fn (callable, optional): Function to calculate the length of the text. Defaults to len.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of chunked text segments.\n",
    "    \"\"\"\n",
    "    # Import the RecursiveCharacterTextSplitter from langchain.text_splitter module\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    \n",
    "    # Instantiate the text splitter with the specified parameters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=length_fn,\n",
    "    )\n",
    "    \n",
    "    # Split the input text into chunks\n",
    "    docs = text_splitter.create_documents([text])\n",
    "    \n",
    "    # Return the list of chunked text segments\n",
    "    return [x.page_content for x in docs]\n",
    "\n",
    "def do_basic_chunking(text, chunk_delimeter=\"\\n\\n\", add_delim_back=False):\n",
    "    \"\"\"\n",
    "    Perform basic chunking on the input text using the specified delimiter.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be chunked.\n",
    "        chunk_delimeter (str, optional): The delimiter used to split the text. Defaults to \"\\n\\n\".\n",
    "        add_delim_back (bool, optional): Whether to add the delimiter back to the end of each chunk. Defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of chunked text segments.\n",
    "    \"\"\"\n",
    "    # Split the input text based on the specified delimiter (ensure no empty chunks by stripping from ends)\n",
    "    docs = text.strip(chunk_delimeter).split(chunk_delimeter)\n",
    "    \n",
    "    # If specified, add the delimiter back to the end of each chunk\n",
    "    if add_delim_back:\n",
    "        docs = [x + chunk_delimeter for x in docs]\n",
    "    \n",
    "    # Return the list of chunked text segments\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f23a54-67a5-4cf8-aa1a-e855b024faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to pass non-default kwargs \n",
    "#    -- otherwise the rcts chunks will overlap by 64 and be 512 characters long\n",
    "CHUNK_STYLE = \"basic\" # one of ['basic' | 'rcts']\n",
    "basic_chunks = do_basic_chunking(ss_text)\n",
    "rcts_chunks = do_rcts_chunking(ss_text)\n",
    "\n",
    "print(\"\\n... FIRST BASIC CHUNK ...\\n\")\n",
    "print(basic_chunks[0])\n",
    "\n",
    "print(\"\\n... FIRST RCTS CHUNK ...\\n\")\n",
    "print(rcts_chunks[0])\n",
    "\n",
    "print(\"\\n... EXAMPLE RANDOM BASIC CHUNK ...\\n\")\n",
    "print(random.sample(basic_chunks, 1)[0])\n",
    "\n",
    "print(\"\\n... EXAMPLE RANDOM RCTS CHUNK ...\\n\")\n",
    "print(random.sample(rcts_chunks, 1)[0])\n",
    "\n",
    "print(\"\\n... LAST BASIC CHUNK ...\\n\")\n",
    "print(basic_chunks[-1])\n",
    "\n",
    "print(\"\\n... LAST RCTS CHUNK ...\\n\")\n",
    "print(rcts_chunks[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57026a1f-c7b1-4d04-892b-0f8505b94ff8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üí¨ Tokenization <a name=\"tokenization\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this section, we'll tokenize the text, which involves splitting the chunks into individual tokens (words or subwords). Tokenization is an essential step in preprocessing, as it helps the Language Model understand the basic units of the text and learn meaningful patterns.\n",
    "\n",
    "* We will train our tokenizer on the non-chunked dataset (after basic preprocessing), however, we will \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517b50e-c416-408b-8480-61ffb558f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model directory if not already setup\n",
    "MODEL_DIR = os.path.join(os.path.dirname(DATA_PATH), \"models\")\n",
    "if not os.path.isdir(MODEL_DIR): os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# User defined parameters (matching alphafold and code tokenization standards)\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, 'spearecode_bpe')\n",
    "VOCAB_SIZE = 8_000\n",
    "CHAR_COVERAGE = 0.99995\n",
    "TOKENIZER_STYLE=\"bpe\"\n",
    "USER_DEFINED_SYMBOLS = [\"\\n\",\"\\t\",\"\\r\",\"\\f\",\"\\v\"]\n",
    "\n",
    "# Tokenizer parameters (and some defaults)\n",
    "tokenizer_kwargs = dict(\n",
    "    input = PREPROCESSED_FULL_TEXT_PATH,\n",
    "    model_prefix=MODEL_PATH,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    character_coverage=CHAR_COVERAGE,\n",
    "    model_type=TOKENIZER_STYLE,\n",
    "    pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
    "    remove_extra_whitespaces=False,\n",
    "    allow_whitespace_only_pieces=True,\n",
    "    add_dummy_prefix=False,\n",
    "    user_defined_symbols=USER_DEFINED_SYMBOLS,\n",
    "    normalization_rule_name=\"identity\"\n",
    ")\n",
    "\n",
    "\n",
    "# train_tokenizer(ALL_TXT_PATHS, MODEL_PATH, VOCAB_SIZE, TOKENIZER_STYLE)\n",
    "spm.SentencePieceTrainer.Train(**tokenizer_kwargs)\n",
    "\n",
    "sp_uni = spm.SentencePieceProcessor().load(f'{MODEL_PATH}.model')\n",
    "sp_bpe = spm.SentencePieceProcessor().load(f'{MODEL_PATH}_bpe.model')\n",
    "\n",
    "uni_encoder = lambda x: sp_uni.encode(x)\n",
    "uni_decoder = lambda x: sp_uni.decode(x)\n",
    "\n",
    "bpe_encoder = lambda x: sp_bpe.encode(x)\n",
    "bpe_decoder = lambda x: sp_bpe.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce0c10-5a5c-4a9a-ba97-d605a0ffc8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "def replace_rightmost_newline(s, replacement='<br>'):\n",
    "    parts = s.rsplit('\\\\n', 1)\n",
    "    return replacement.join(parts)\n",
    "\n",
    "\n",
    "def get_color(value, cmap='Pastel1', transparency=0.5):\n",
    "    \"\"\"\n",
    "    Returns an HTML-formatted string representing the background color for a token.\n",
    "\n",
    "    Args:\n",
    "        value (int): The index of the token to color.\n",
    "        cmap (str, optional): The name of the colormap to use. Defaults to 'Pastel1'.\n",
    "\n",
    "    Returns:\n",
    "        str: An HTML-formatted string representing the background color.\n",
    "    \"\"\"\n",
    "    colormap = plt.get_cmap(cmap)\n",
    "    return f\"background-color: rgba{tuple([int(x*255) for x in colormap(value % colormap.N)[:-1]]+[transparency,])};\"\n",
    "\n",
    "\n",
    "def get_visualization1(tokens, decoder=None, cmap='Pastel1', font_family='Courier New',\n",
    "                       transparency=0.75, font_size='1.1em', unk_token='???', font_weight=300, padding='0px',\n",
    "                       margin_right='0px', border_radius='0px', display_inline=False):\n",
    "    \"\"\"\n",
    "    Generates an HTML string to visualize the tokenization of a text.\n",
    "\n",
    "    Args:\n",
    "        tokens (list):\n",
    "            ‚Äì A list of integer tokens.\n",
    "        decoder (function, optional):\n",
    "            ‚Äì A function that maps an integer to the representative string\n",
    "            ‚Äì If this is None, the tokens are assumed to be strings not integers\n",
    "        cmap (str, optional):\n",
    "            ‚Äì The name of the colormap to use. Defaults to 'Pastel1'.\n",
    "        font_family (str, optional):\n",
    "            ‚Äì The font family to use for tokens. Defaults to 'Courier New'.\n",
    "        transparency (float, optional):\n",
    "            background transparency\n",
    "        font_size (str, optional):\n",
    "            ‚Äì The font size to use for tokens. Defaults to '1.1em'.\n",
    "        unk_token (str, optional):\n",
    "            ‚Äì The string to use for unknown tokens. Defaults to '???'\n",
    "        font_weight (str, optional):\n",
    "            ‚Äì The font weight to use for tokens. Defaults to 'bold'.\n",
    "        padding (str, optional):\n",
    "            ‚Äì The padding to use for tokens. Defaults to '2px'.\n",
    "        margin_right (str, optional):\n",
    "            ‚Äì The right margin to use for tokens. Defaults to '5px'.\n",
    "        border_radius (str, optional):\n",
    "            ‚Äì The border radius to use for tokens. Defaults to '3px'.\n",
    "        display_inline (bool, optional):\n",
    "            ‚Äì Whether to display the HTML inline. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        str: An HTML string representing the tokenized text with styling.\n",
    "    \"\"\"\n",
    "    html = f\"<style>span.token {{font-family: {font_family} !important; font-size: {font_size} !important; font-weight: {font_weight} !important; \" \\\n",
    "           f\"padding: {padding} !important; margin-right: {margin_right} !important; border-radius: {border_radius} !important;}}</style>\"\n",
    "\n",
    "    html += \"<div style='background-color: #F8F8F8; padding: 15px; border-radius: 5px;'>\"\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        color = get_color(i, cmap, transparency)\n",
    "        try:\n",
    "            html += f\"<span class='token' style='{color}'>{decoder(token).replace(' ', '&nbsp;')}</span>\"\n",
    "        except TypeError:\n",
    "            html += f\"<span class='token' style='{color}'>{unk_token}</span>\"\n",
    "\n",
    "    html += \"</div>\"\n",
    "    \n",
    "    if display_inline:\n",
    "        HTML(html)\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "def get_line_viz(token_lines, decoder, cmap='Pastel1', font_family='Courier New',\n",
    "                 transparency=0.75, font_size='1.1em', unk_token='???', font_weight=300, padding='0px',\n",
    "                 margin_right='0px', border_radius='0px', display_inline=False):\n",
    "    \"\"\"\n",
    "    Generates an HTML string to visualize the tokenization of a text.\n",
    "\n",
    "    Args:\n",
    "        token_lines (list):\n",
    "            ‚Äì A list of lists of integer tokens.\n",
    "        decoder (function, optional):\n",
    "            ‚Äì A function that maps an integer to the representative string\n",
    "            ‚Äì If this is None, the tokens are assumed to be strings not integers\n",
    "        cmap (str, optional):\n",
    "            ‚Äì The name of the colormap to use. Defaults to 'Pastel1'.\n",
    "        font_family (str, optional):\n",
    "            ‚Äì The font family to use for tokens. Defaults to 'Courier New'.\n",
    "        transparency (float, optional):\n",
    "            background transparency\n",
    "        font_size (str, optional):\n",
    "            ‚Äì The font size to use for tokens. Defaults to '1.1em'.\n",
    "        unk_token (str, optional):\n",
    "            ‚Äì The string to use for unknown tokens. Defaults to '???'\n",
    "        font_weight (str, optional):\n",
    "            ‚Äì The font weight to use for tokens. Defaults to 'bold'.\n",
    "        padding (str, optional):\n",
    "            ‚Äì The padding to use for tokens. Defaults to '2px'.\n",
    "        margin_right (str, optional):\n",
    "            ‚Äì The right margin to use for tokens. Defaults to '5px'.\n",
    "        border_radius (str, optional):\n",
    "            ‚Äì The border radius to use for tokens. Defaults to '3px'.\n",
    "        display_inline (bool, optional):\n",
    "            ‚Äì Whether to display the HTML inline. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        str: An HTML string representing the tokenized text with styling.\n",
    "    \"\"\"\n",
    "    html = f\"<style>span.token {{font-family: {font_family} !important; font-size: {font_size} !important; font-weight: {font_weight} !important; \" \\\n",
    "           f\"padding: {padding} !important; margin-right: {margin_right} !important; border-radius: {border_radius} !important;}}</style>\"\n",
    "\n",
    "    html += \"<div style='background-color: #F8F8F8; padding: 15px; border-radius: 5px;'>\"\n",
    "    \n",
    "    for token_line in token_lines:\n",
    "        for i, token in enumerate(token_line):\n",
    "            color = get_color(i, cmap, transparency)\n",
    "            try:\n",
    "                html += f\"<span class='token' style='{color}'>{decoder(token).replace(' ', '&nbsp;')}</span>\".replace('\\t', '\\\\t').replace('\\n', '\\\\n').replace('\\r', '\\\\r').replace('\\f', '\\\\f').replace('\\v', '\\\\v')\n",
    "            except TypeError:\n",
    "                html += f\"<span class='token' style='{color}'>{unk_token}</span>\"\n",
    "        html = replace_rightmost_newline(html)\n",
    "    html += \"</div>\"\n",
    "    \n",
    "    if display_inline:\n",
    "        HTML(html)\n",
    "\n",
    "    return html\n",
    "\n",
    "def plot_tokenization(text, encoder, decoder, split_on=\"\\n\"):\n",
    "    display(HTML(get_line_viz([encoder(x+split_on) for x in text.split(split_on)], decoder)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039baa8-358d-4f36-b045-b86b9f6d5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n... BPE TOKENIZATION:\")\n",
    "plot_tokenization(basic_chunks[0], bpe_encoder, bpe_decoder)\n",
    "\n",
    "print(\"\\n... UNIGRAM TOKENIZATION:\")\n",
    "plot_tokenization(basic_chunks[0], uni_encoder, uni_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b78d20-17c7-4e7e-9f80-155aa86089cc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üìä Basic Exploratory Data Analysis (EDA) <a name=\"basic-eda\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Here, we'll perform a basic EDA on the dataset to gain insights and identify potential issues. This analysis may include examining token frequency, distribution of chunk lengths, and other relevant characteristics. This information can be helpful in understanding the dataset's structure and guiding further preprocessing decisions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aabee3-7e10-4898-906b-9d663529afca",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üíæ Converting to TFRecords <a name=\"converting-to-tfrecords\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Finally, after completing the preprocessing steps and EDA, we'll convert the toy dataset into the `TFRecords` format. This efficient binary format is designed for use with TensorFlow and will enable seamless integration with your Language Model training pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462afc7-0760-4544-9455-86f22e9ebe7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049fe4cb-d12f-49c9-8bdc-c663eaa764f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
