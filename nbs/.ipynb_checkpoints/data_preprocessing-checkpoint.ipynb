{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29279551-2a6d-480f-8fe2-d18017f56b20",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# üöÄ Spearecode Preprocessing üöÄ\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Welcome to the **Spearecode Preprocessing Notebook**! This notebook will guide you through the necessary preprocessing steps to prepare a toy dataset for Language Model training. We will focus on making the dataset more suitable for training by performing the following steps:\n",
    "\n",
    "1. üìö **Loading the dataset**: We'll start by importing the dataset from a file or external source.\n",
    "2. üì¶ **Chunking the text**: The dataset will be divided into smaller chunks or segments, making it easier to process during training.\n",
    "3. üí¨ **Tokenization**: Each chunk of text will be split into individual tokens (words or subwords), which are the basic units for language models.\n",
    "4. üìä **Basic Exploratory Data Analysis (EDA)**: We'll analyze the dataset's characteristics, such as token frequency, to gain insights and identify potential issues.\n",
    "\n",
    "After completing the preprocessing and EDA, the toy dataset will be converted into `TFRecords` format. This efficient binary format is designed for use with TensorFlow and will enable seamless integration with your Language Model training pipeline.\n",
    "\n",
    "Let's dive in and start preprocessing the dataset! üéâ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a62943-0d3c-4aae-933b-2a5fbe751ad6",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "# üåü Table of Contents üåü\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "0. [**Setup**](#setup)\n",
    "1. [**Loading the Dataset**](#loading-the-dataset)\n",
    "2. [**Chunking the Text**](#chunking-the-text)\n",
    "3. [**Tokenization**](#tokenization)\n",
    "4. [**Basic Exploratory Data Analysis (EDA)**](#basic-eda)\n",
    "5. [**Converting to TFRecords**](#converting-to-tfrecords)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e83729-e61a-4d94-a074-17ccb60593dc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üõ†Ô∏è Setup <a name=\"setup\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this section, we'll import required libraries and methods from our utilities file. We will also define relevant paths and high level information we may need later. We also run a few basic Tensorflow setup steps to ensure optimal and reproducible runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab4c015-345e-4c48-a749-c7094604fecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tokenizer-viz\n",
    "\n",
    "# Regular imports (native python and pypi packages)\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import sentencepiece as spm\n",
    "from tokenizer_viz import TokenVisualization\n",
    "from tqdm.notebook import tqdm; tqdm.pandas()\n",
    "\n",
    "# Add project root into path so imports work\n",
    "PROJECT_DIR = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, PROJECT_DIR) \n",
    "\n",
    "# Our project imports\n",
    "from spearecode.preprocessing_utils import (\n",
    "    load_from_txt_file, preprocess_shakespeare, save_to_txt_file, print_check_speare\n",
    ")\n",
    "from spearecode.general_utils import (\n",
    "    tf_xla_jit, tf_set_memory_growth, seed_it_all, flatten_l_o_l, print_ln\n",
    ")\n",
    "from spearecode.filtering_utils import (\n",
    "    save_ds_version, drop_str_from_col_names, pad_truncate_centered,\n",
    "    get_metadata_df, check_chunks, tokenize, get_n_tokens,\n",
    "    get_n_lines, get_n_chars\n",
    ")\n",
    "from spearecode.tfrecord_utils import write_tfrecords\n",
    "\n",
    "\n",
    "### DEFINE PATHS --- [PROJECT_DIR=\"/home/paperspace/home/spearecode\"] --- ###\n",
    "NBS_PATH = os.path.join(PROJECT_DIR, \"nbs\")\n",
    "DATA_PATH = os.path.join(PROJECT_DIR, \"data\")\n",
    "SS_TEXT_PATH = os.path.join(DATA_PATH, \"t8.shakespeare.txt\")\n",
    "PREPROCESSED_FULL_TEXT_PATH = SS_TEXT_PATH.replace(\".txt\", \"_preprocessed.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3975cca8-555a-4871-a41a-3ba37c72aa12",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üìö Loading the Dataset <a name=\"loading-the-dataset\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this section, we'll import the dataset from a file or external source. The dataset will be read into memory, allowing us to manipulate and process the text as needed throughout the preprocessing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7fa905-5791-48de-9933-bcd0fcfe95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = load_from_txt_file(SS_TEXT_PATH)\n",
    "ss_text = preprocess_shakespeare(raw_text)\n",
    "save_to_txt_file(ss_text, PREPROCESSED_FULL_TEXT_PATH)\n",
    "print_check_speare(ss_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ead5a1-5ce2-448f-83fd-7c38cd7ba060",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üì¶ Chunking the Text <a name=\"chunking-the-text\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Once the dataset is loaded, we'll divide it into smaller chunks or segments. This step is crucial for making the dataset more similar to code files (which is the type of data we will be using during the other parallel streams).\n",
    "\n",
    "I implement two simple methods:\n",
    "1. A basic double newline split **(`\\n\\n`)** resulting in 6294 chunks\n",
    "2. Using Langchain RecursiveTextSplitter to chunk to a particular text length\n",
    "    * This allows us to specify our desired text length and even overlap the chunks.\n",
    "        * Note we allow for a small amount of overlap and this may cause some leakage... but whatever.\n",
    "    * **We will use this method for our purposes.**\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8bb3c7-0144-4bb9-ba56-97e91893fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_rcts_chunking(text, chunk_size=1024, chunk_overlap=128, length_fn=len):\n",
    "    \"\"\"\n",
    "    Perform Recursive Character Text Splitting (RCTS) chunking on the input text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be chunked.\n",
    "        chunk_size (int): The maximum size of each chunk.\n",
    "        chunk_overlap (int): The number of overlapping characters between adjacent chunks.\n",
    "        length_fn (callable, optional): Function to calculate the length of the text. Defaults to len.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of chunked text segments.\n",
    "    \"\"\"\n",
    "    # Import the RecursiveCharacterTextSplitter from langchain.text_splitter module\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    \n",
    "    # Instantiate the text splitter with the specified parameters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=length_fn,\n",
    "    )\n",
    "    \n",
    "    # Split the input text into chunks\n",
    "    docs = text_splitter.create_documents([text])\n",
    "    \n",
    "    # Return the list of chunked text segments\n",
    "    return [x.page_content for x in docs if len(x.page_content)>1]\n",
    "\n",
    "def do_basic_chunking(text, chunk_delimeter=\"\\n\\n\", max_length=1800, min_length=300):\n",
    "    \"\"\"\n",
    "    Perform basic chunking on the input text using the specified delimiter.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to be chunked.\n",
    "        chunk_delimeter (str, optional): The delimiter used to split the text. Defaults to \"\\n\\n\".\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of chunked text segments.\n",
    "    \"\"\"\n",
    "    # Split the input text based on the specified delimiter (ensure no empty chunks by stripping from ends)\n",
    "    raw_docs = text.strip(chunk_delimeter).split(chunk_delimeter)\n",
    "    tmp_docs = []\n",
    "    docs = []\n",
    "    \n",
    "    while len(raw_docs)>0:\n",
    "        doc = raw_docs.pop()\n",
    "        \n",
    "        if len(doc)>max_length:\n",
    "            raw_docs+=doc.split(\"\\n\")\n",
    "        elif len(doc)<min_length:\n",
    "            tmp_docs.append(doc)\n",
    "        else:\n",
    "            docs.append(doc)\n",
    "            \n",
    "        if len(\"\\n\".join(tmp_docs))>min_length:\n",
    "            docs.append(\"\\n\".join(tmp_docs))\n",
    "            tmp_docs = []\n",
    "    if tmp_docs:\n",
    "        docs.append(\"\\n\".join(tmp_docs))\n",
    "    \n",
    "    # Return the list of chunked text segments\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f23a54-67a5-4cf8-aa1a-e855b024faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to pass non-default kwargs \n",
    "#    -- otherwise the rcts chunks will overlap by 64 and be 512 characters long\n",
    "CHUNK_STYLE = \"basic\" # one of ['basic' | 'rcts']\n",
    "basic_chunks = do_basic_chunking(ss_text)\n",
    "rcts_chunks = do_rcts_chunking(ss_text)\n",
    "\n",
    "print(\"\\n... FIRST BASIC CHUNK ...\\n\")\n",
    "print(basic_chunks[0])\n",
    "\n",
    "print(\"\\n... FIRST RCTS CHUNK ...\\n\")\n",
    "print(rcts_chunks[0])\n",
    "\n",
    "print(\"\\n... EXAMPLE RANDOM BASIC CHUNK ...\\n\")\n",
    "print(random.sample(basic_chunks, 1)[0])\n",
    "\n",
    "print(\"\\n... EXAMPLE RANDOM RCTS CHUNK ...\\n\")\n",
    "print(random.sample(rcts_chunks, 1)[0])\n",
    "\n",
    "print(\"\\n... LAST BASIC CHUNK ...\\n\")\n",
    "print(basic_chunks[-1])\n",
    "\n",
    "print(\"\\n... LAST RCTS CHUNK ...\\n\")\n",
    "print(rcts_chunks[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57026a1f-c7b1-4d04-892b-0f8505b94ff8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üí¨ Tokenization <a name=\"tokenization\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "In this section, we'll tokenize the text, which involves splitting the chunks into individual tokens (words or subwords). Tokenization is an essential step in preprocessing, as it helps the Language Model understand the basic units of the text and learn meaningful patterns.\n",
    "\n",
    "* We will train our tokenizer on the non-chunked dataset (after basic preprocessing), however, we will \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4517b50e-c416-408b-8480-61ffb558f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup model directory if not already setup\n",
    "MODEL_DIR = os.path.join(os.path.dirname(DATA_PATH), \"models\")\n",
    "if not os.path.isdir(MODEL_DIR): os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# User defined parameters (matching alphafold and code tokenization standards)\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, 'spearecode')\n",
    "USER_DEFINED_SYMBOLS = [\"\\n\",\"\\t\",\"\\r\",\"\\f\",\"\\v\"]\n",
    "VOCAB_SIZE = 8_000\n",
    "CHAR_COVERAGE = 1.0000\n",
    "\n",
    "# Tokenizer parameters (and some defaults)\n",
    "base_tokenizer_kwargs = dict(\n",
    "    input = PREPROCESSED_FULL_TEXT_PATH,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    character_coverage=CHAR_COVERAGE,\n",
    "    pad_id=0, unk_id=1, bos_id=2, eos_id=3,\n",
    "    remove_extra_whitespaces=False,\n",
    "    allow_whitespace_only_pieces=True,\n",
    "    add_dummy_prefix=False,\n",
    "    user_defined_symbols=USER_DEFINED_SYMBOLS,\n",
    "    normalization_rule_name=\"identity\",\n",
    "    num_threads=os.cpu_count(),\n",
    ")\n",
    "\n",
    "unigram_tokenizer_kwargs = base_tokenizer_kwargs.copy()\n",
    "unigram_tokenizer_kwargs.update(dict(\n",
    "    model_prefix=MODEL_PATH+\"_unigram\",\n",
    "    model_type=\"unigram\",\n",
    "))\n",
    "\n",
    "bpe_tokenizer_kwargs = base_tokenizer_kwargs.copy()\n",
    "bpe_tokenizer_kwargs.update(dict(\n",
    "    model_prefix=MODEL_PATH+\"_bpe\",\n",
    "    model_type=\"bpe\",\n",
    "))\n",
    "\n",
    "# train_tokenizer(ALL_TXT_PATHS, MODEL_PATH, VOCAB_SIZE, TOKENIZER_STYLE)\n",
    "spm.SentencePieceTrainer.Train(**unigram_tokenizer_kwargs)\n",
    "spm.SentencePieceTrainer.Train(**bpe_tokenizer_kwargs)\n",
    "\n",
    "sp_uni = spm.SentencePieceProcessor()\n",
    "sp_uni.load(f'{unigram_tokenizer_kwargs[\"model_prefix\"]}.model')\n",
    "uni_encoder = lambda x: sp_uni.encode(x)\n",
    "uni_decoder = lambda x: sp_uni.decode(x)\n",
    "\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "sp_bpe.load(f'{bpe_tokenizer_kwargs[\"model_prefix\"]}.model')\n",
    "bpe_encoder = lambda x: sp_bpe.encode(x)\n",
    "bpe_decoder = lambda x: sp_bpe.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9039baa8-358d-4f36-b045-b86b9f6d5743",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n... BPE TOKENIZATION:\")\n",
    "bpe_token_viz = TokenVisualization(\n",
    "    encoder=bpe_encoder,\n",
    "    decoder=bpe_decoder,\n",
    "    background_color=\"#FBFBFB\"\n",
    ")\n",
    "_ = bpe_token_viz.visualize(basic_chunks[0], display_inline=True)\n",
    "\n",
    "print(\"\\n... UNIGRAM TOKENIZATION:\")\n",
    "uni_token_viz = TokenVisualization(\n",
    "    encoder=uni_encoder,\n",
    "    decoder=uni_decoder,\n",
    "    background_color=\"#FBFBFB\"\n",
    ")\n",
    "_ = uni_token_viz.visualize(basic_chunks[0], display_inline=True)\n",
    "\n",
    "\n",
    "### FOR FUN AND VIZ ###\n",
    "print(\"\\n... CHAR TOKENIZATION:\")\n",
    "dumb_char_map_s2i = {x:i for i, x in enumerate(set(basic_chunks[0]))}\n",
    "dumb_char_map_i2s = {v:k for k,v in dumb_char_map_s2i.items()}\n",
    "char_token_viz = TokenVisualization(\n",
    "    encoder=lambda x: [dumb_char_map_s2i.get(_x) for _x in x] if type(x)==str else dumb_char_map_s2i.get(x),\n",
    "    decoder=lambda x: \"\".join([dumb_char_map_i2s.get(_x) for _x in x]) if type(x)==list else dumb_char_map_i2s.get(x),\n",
    "    background_color=\"#FBFBFB\"\n",
    ")\n",
    "_ = char_token_viz.visualize(basic_chunks[0], display_inline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b78d20-17c7-4e7e-9f80-155aa86089cc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üìä Basic Exploratory Data Analysis (EDA) <a name=\"basic-eda\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Here, we'll perform a basic EDA on the dataset to gain insights and identify potential issues. This analysis may include examining token frequency, distribution of chunk lengths, and other relevant characteristics. This information can be helpful in understanding the dataset's structure and guiding further preprocessing decisions.\n",
    "\n",
    "We will utilize the metadata columns we create to create different versions of the dataset:\n",
    "\n",
    "<br>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th style=\"text-align: center; font-weight: bold; width: 15%;\">Version</th>\n",
    "      <th style=\"text-align: center; font-weight: bold; width: 85%;\">Description</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td style=\"text-align: center;\"><strong>v1</strong></td>\n",
    "      <td>No filtering, no chunks removed (the above dataframe)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align: center;\"><strong>v2</strong></td>\n",
    "      <td>Split into individual datasets for bpe, unigram within each chunking technique (4 datasets total)<br>Rename columns to not specify tokenization method to allow for more generalization across interaction</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align: center;\"><strong>v3</strong></td>\n",
    "      <td>Drop small chunks<br>Drop really big chunks</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td style=\"text-align: center;\"><strong>v4</strong></td>\n",
    "      <td>Evenly pad/truncated tokenized sequences up to reasonable length (close to max length --> 90th percentile?)</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92289f97-bf0d-42bb-85c9-5ffeb3259694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup - Create directories if necessary    \n",
    "DATASET_DIR = os.path.join(DATA_PATH, \"datasets\")\n",
    "if not os.path.isdir(DATASET_DIR): os.makedirs(DATASET_DIR, exist_ok=True)\n",
    "META_DIR = os.path.join(DATASET_DIR, \"meta\")\n",
    "if not os.path.isdir(META_DIR): os.makedirs(META_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# 2. Dataframe and metadata creation\n",
    "#       - Instantiate\n",
    "#       - Create metadata columns\n",
    "#       - Create metadata dataframe (optional)\n",
    "basic_chunk_df = pd.DataFrame({\"content\":basic_chunks})\n",
    "rcts_chunk_df = pd.DataFrame({\"content\":rcts_chunks})\n",
    "\n",
    "for _df in [basic_chunk_df, rcts_chunk_df]:\n",
    "    _df[\"uni_token_content\"] = _df[\"content\"].progress_apply(lambda x: tokenize(x, uni_encoder))\n",
    "    _df[\"bpe_token_content\"] = _df[\"content\"].progress_apply(lambda x: tokenize(x, bpe_encoder))\n",
    "    _df[\"n_uni_tokens\"] = _df[\"uni_token_content\"].apply(get_n_tokens)\n",
    "    _df[\"n_bpe_tokens\"] = _df[\"bpe_token_content\"].apply(get_n_tokens)\n",
    "    _df[\"n_chars\"] = _df[\"content\"].apply(get_n_chars)\n",
    "    _df[\"n_lines\"] = _df[\"content\"].apply(get_n_lines)\n",
    "    _df[\"valid_uni_chunk\"] = _df[\"n_uni_tokens\"].apply(check_chunks)\n",
    "    _df[\"valid_bpe_chunk\"] = _df[\"n_bpe_tokens\"].apply(check_chunks)\n",
    "\n",
    "basic_chunk_df_meta = get_metadata_df(basic_chunk_df)\n",
    "rcts_chunk_df_meta = get_metadata_df(rcts_chunk_df)\n",
    "\n",
    "\n",
    "# 3. Versioning\n",
    "    \n",
    "######################################## v1 ########################################\n",
    "# Save the previously created datasets along with the manually created metadata\n",
    "####################################################################################\n",
    "save_ds_version(rcts_chunk_df, \"rcts\", version_str=\"v1\", meta_dir=META_DIR, ds_dir=DATASET_DIR, meta_df=rcts_chunk_df_meta)\n",
    "save_ds_version(basic_chunk_df, \"basic\", version_str=\"v1\", meta_dir=META_DIR, ds_dir=DATASET_DIR, meta_df=basic_chunk_df_meta)\n",
    "####################################################################################\n",
    "\n",
    "######################################## v2 ########################################\n",
    "# Split bpe and unigram into their own dataframes (meta is generated automatically)\n",
    "####################################################################################\n",
    "rcts_uni_chunk_df = rcts_chunk_df.copy().drop(columns=[_c for _c in rcts_chunk_df.columns if \"bpe\" in _c])\n",
    "basic_uni_chunk_df = basic_chunk_df.copy().drop(columns=[_c for _c in basic_chunk_df.columns if \"bpe\" in _c])\n",
    "rcts_bpe_chunk_df = rcts_chunk_df.copy().drop(columns=[_c for _c in rcts_chunk_df.columns if \"uni\" in _c])\n",
    "basic_bpe_chunk_df = basic_chunk_df.copy().drop(columns=[_c for _c in basic_chunk_df.columns if \"uni\" in _c])\n",
    "\n",
    "# Rename columns\n",
    "rcts_uni_chunk_df = drop_str_from_col_names(rcts_uni_chunk_df, \"uni\")\n",
    "rcts_bpe_chunk_df = drop_str_from_col_names(rcts_bpe_chunk_df, \"bpe\")\n",
    "basic_uni_chunk_df = drop_str_from_col_names(basic_uni_chunk_df, \"uni\")\n",
    "basic_bpe_chunk_df = drop_str_from_col_names(basic_bpe_chunk_df, \"bpe\")\n",
    "\n",
    "save_ds_version(rcts_uni_chunk_df, \"rcts_uni\", version_str=\"v2\", meta_dir=META_DIR, ds_dir=DATASET_DIR)\n",
    "save_ds_version(rcts_bpe_chunk_df, \"rcts_bpe\", version_str=\"v2\", meta_dir=META_DIR, ds_dir=DATASET_DIR)\n",
    "save_ds_version(basic_uni_chunk_df, \"basic_uni\", version_str=\"v2\", meta_dir=META_DIR, ds_dir=DATASET_DIR)\n",
    "save_ds_version(basic_bpe_chunk_df, \"basic_bpe\", version_str=\"v2\", meta_dir=META_DIR, ds_dir=DATASET_DIR)\n",
    "####################################################################################\n",
    "\n",
    "######################################## v3 ########################################\n",
    "# Filtering (big and little get dropped)\n",
    "####################################################################################\n",
    "\n",
    "# Filter and drop valid chunk col\n",
    "for _df in [rcts_uni_chunk_df, rcts_bpe_chunk_df, basic_uni_chunk_df, basic_bpe_chunk_df]:\n",
    "    _df = _df[_df.valid_chunk].drop(columns=[\"valid_chunk\"]).reset_index(drop=True)\n",
    "\n",
    "# Save\n",
    "save_ds_version(rcts_uni_chunk_df, \"rcts_uni\", version_str=\"v3\", meta_dir=META_DIR, ds_dir=DATASET_DIR)\n",
    "save_ds_version(rcts_bpe_chunk_df, \"rcts_bpe\", version_str=\"v3\", meta_dir=META_DIR, ds_dir=DATASET_DIR)\n",
    "save_ds_version(basic_uni_chunk_df, \"basic_uni\", version_str=\"v3\", meta_dir=META_DIR, ds_dir=DATASET_DIR)\n",
    "save_ds_version(basic_bpe_chunk_df, \"basic_bpe\", version_str=\"v3\", meta_dir=META_DIR, ds_dir=DATASET_DIR)\n",
    "####################################################################################\n",
    "\n",
    "# ######################################## v4 ########################################\n",
    "# # Padding and truncation --> basic upper limit of 384 (assuming context lengths of 64-128)\n",
    "# ####################################################################################\n",
    "FIXED_CHUNK_SIZE = 384\n",
    "\n",
    "for _df in [rcts_uni_chunk_df, rcts_bpe_chunk_df, basic_uni_chunk_df, basic_bpe_chunk_df]:\n",
    "    _df[\"token_content\"] = _df[\"token_content\"].apply(lambda x: pad_truncate_centered(x, FIXED_CHUNK_SIZE))\n",
    "\n",
    "save_ds_version(rcts_uni_chunk_df, \"rcts_uni\", version_str=\"v4\", meta_dir=META_DIR, ds_dir=DATASET_DIR)\n",
    "save_ds_version(rcts_bpe_chunk_df, \"rcts_bpe\", version_str=\"v4\", meta_dir=META_DIR, ds_dir=DATASET_DIR)\n",
    "save_ds_version(basic_uni_chunk_df, \"basic_uni\", version_str=\"v4\", meta_dir=META_DIR, ds_dir=DATASET_DIR)\n",
    "save_ds_version(basic_bpe_chunk_df, \"basic_bpe\", version_str=\"v4\", meta_dir=META_DIR, ds_dir=DATASET_DIR)\n",
    "# ####################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aabee3-7e10-4898-906b-9d663529afca",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "## üíæ Converting to TFRecords <a name=\"converting-to-tfrecords\"></a>\n",
    "\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "Finally, after completing the preprocessing steps and EDA, we'll convert the toy dataset into the `TFRecords` format. This efficient binary format is designed for use with TensorFlow and will enable seamless integration with your Language Model training pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462afc7-0760-4544-9455-86f22e9ebe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tfrecord creation constants\n",
    "TFRECORD_DIR = os.path.join(DATASET_DIR, \"tfrecords\")\n",
    "N_PER = 1000 # artificially low to replicate tfrecord amounts expected\n",
    "VERSION_TO_USE = \"v4\"\n",
    "\n",
    "for _df, _suffix in zip([rcts_uni_chunk_df, rcts_bpe_chunk_df, basic_uni_chunk_df, basic_bpe_chunk_df], \n",
    "                        [\"rcts_uni\", \"rcts_bpe\", \"basic_uni\", \"basic_bpe\"]):\n",
    "    # Create the respective tfrecords\n",
    "    write_tfrecords(\n",
    "        ds=rcts_uni_chunk_df['token_content'],  n_ex=len(_df), \n",
    "        output_suffix=_suffix,  version_str=VERSION_TO_USE, \n",
    "        n_ex_per_rec=N_PER, out_dir=TFRECORD_DIR, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0075889a-00b8-4d08-ac86-b68f5868acb3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Check dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2179a-823a-41a8-a2cd-f753a66f5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_example(example_proto, max_seq_len=384, default_value=0):\n",
    "    feature_map = {\n",
    "        'token_content': tf.io.FixedLenFeature(\n",
    "            shape=[max_seq_len], dtype=tf.int64, default_value=[default_value]*max_seq_len\n",
    "        ) # tf.io.VarLenFeature(tf.int64),\n",
    "    }\n",
    "    features = tf.io.parse_single_example(example_proto, features=feature_map)['token_content']\n",
    "    \n",
    "    ### HOW FOR VARIABLE\n",
    "    # tf.io.VarLenFeature(tf.int64),\n",
    "    #parsed_example = tf.io.parse_single_example(example_proto, feature_map)\n",
    "    #features = tf.sparse.to_dense(parsed_example['token_content'])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def load_tfrecord_dataset(input_files):\n",
    "    raw_dataset = tf.data.TFRecordDataset(input_files)\n",
    "    parsed_dataset = raw_dataset.map(parse_example)\n",
    "    return parsed_dataset\n",
    "\n",
    "DEMO_DS_CHUNK_STYLE, DEMO_DS_TOK_STYLE, DEMO_DS_VERSION = \"rcts\", \"bpe\", \"v4\"\n",
    "DEMO_TFREC_PATHS = sorted(glob(os.path.join(\n",
    "    TFRECORD_DIR, DEMO_Df\"{DEMO_DS_CHUNK_STYLE}_{DEMO_DS_TOK_STYLE}_{DEMO_DS_VERSION}\"S_STR, \"*.tfrec\"\n",
    ")))\n",
    "DEMO_ENCODER = bpe_encoder if DEMO_DS_TOK_STYLE==\"bpe\" else uni_encoder\n",
    "DEMO_DECODER = bpe_decoder if DEMO_DS_TOK_STYLE==\"bpe\" else uni_decoder\n",
    "demo_ds = load_tfrecord_dataset(DEMO_TFREC_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38ce19-3dd9-47e4-9546-0fe8f37f3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n... FROM TFRECORD ...\\n\")\n",
    "plot_tokenization(bpe_decoder(next(iter(demo_ds)).numpy().tolist()), DEMO_ENCODER, DEMO_DECODER)\n",
    "\n",
    "# TODO make modular... not important\n",
    "print(\"\\n... FROM PANDAS DATAFRAME ...\\n\")\n",
    "plot_tokenization(bpe_decoder(rcts_bpe_chunk_df[\"token_content\"][0]), DEMO_ENCODER, DEMO_DECODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad0b2ae-1443-4fe4-b74d-b29344334da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
